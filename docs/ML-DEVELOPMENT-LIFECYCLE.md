# ML Development Lifecycle: From Experimentation to Production

## Overview

This document describes the complete machine learning development lifecycle, from initial experimentation in notebooks to automated production pipelines. It provides real-world examples from leading tech companies and explains how experimentation and production automation work together.

## Table of Contents

1. [The Complete ML Lifecycle](#the-complete-ml-lifecycle)
2. [Phase 1: Experimentation (Notebooks)](#phase-1-experimentation-notebooks)
3. [Phase 2: Production Deployment (Automated Pipelines)](#phase-2-production-deployment-automated-pipelines)
4. [Phase 3: Monitoring & Drift Detection](#phase-3-monitoring--drift-detection)
5. [Phase 4: Continuous Improvement](#phase-4-continuous-improvement)
6. [Real Company Examples](#real-company-examples)
7. [Integration Patterns](#integration-patterns)

---

## The Complete ML Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: EXPERIMENTATION (Notebooks)                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Data exploration and analysis                                â”‚
â”‚  â€¢ Feature engineering experiments                              â”‚
â”‚  â€¢ Algorithm selection (XGBoost, LightGBM, Neural Networks)     â”‚
â”‚  â€¢ Hyperparameter tuning (grid search, random search, Bayesian) â”‚
â”‚  â€¢ Model evaluation and comparison                              â”‚
â”‚  â€¢ A/B testing preparation                                      â”‚
â”‚                                                                  â”‚
â”‚  Tools: Jupyter, SageMaker Studio, Databricks                   â”‚
â”‚  Duration: Days to weeks                                        â”‚
â”‚  Output: Best model configuration + hyperparameters             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: PRODUCTION DEPLOYMENT (Automated Pipelines)           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Codify winning configuration from experiments                â”‚
â”‚  â€¢ Build automated training pipeline                            â”‚
â”‚  â€¢ Implement validation gates (accuracy thresholds)             â”‚
â”‚  â€¢ Set up scheduled retraining (daily/weekly/monthly)           â”‚
â”‚  â€¢ Deploy to production endpoints                               â”‚
â”‚                                                                  â”‚
â”‚  Tools: Step Functions, Lambda, SageMaker Training Jobs         â”‚
â”‚  Duration: Ongoing (automated)                                  â”‚
â”‚  Output: Production-ready models, continuously updated          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: MONITORING & DRIFT DETECTION                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Track model performance metrics                              â”‚
â”‚  â€¢ Detect data drift (input distribution changes)               â”‚
â”‚  â€¢ Detect concept drift (relationship changes)                  â”‚
â”‚  â€¢ Alert when performance degrades                              â”‚
â”‚  â€¢ Trigger re-experimentation when needed                       â”‚
â”‚                                                                  â”‚
â”‚  Tools: CloudWatch, Custom monitoring, MLflow                   â”‚
â”‚  Duration: Continuous                                           â”‚
â”‚  Output: Alerts, performance reports, drift signals             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 4: CONTINUOUS IMPROVEMENT (Back to Notebooks)            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Investigate performance degradation                          â”‚
â”‚  â€¢ Experiment with new features                                 â”‚
â”‚  â€¢ Try new algorithms or architectures                          â”‚
â”‚  â€¢ Fine-tune hyperparameters for new data patterns              â”‚
â”‚  â€¢ Update production pipeline with improvements                 â”‚
â”‚                                                                  â”‚
â”‚  Tools: Jupyter, SageMaker Studio                               â”‚
â”‚  Duration: Triggered by monitoring or scheduled quarterly       â”‚
â”‚  Output: Updated model configuration â†’ Deploy to production     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    (Cycle repeats)
```

---

## Phase 1: Experimentation (Notebooks)

### Purpose
Data scientists explore, experiment, and discover the best model configuration before committing to production automation.

### Key Activities

#### 1. Data Exploration
```python
# In SageMaker Notebook or Jupyter
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load and explore data
df = pd.read_csv('s3://fraud-detection-data/kaggle-credit-card-fraud.csv')

# Understand the data
print(df.describe())
print(df['Class'].value_counts())  # Check class imbalance

# Visualize distributions
sns.histplot(df['Amount'])
plt.show()

# Check for missing values
print(df.isnull().sum())
```

#### 2. Feature Engineering
```python
# Create new features
df['hour'] = pd.to_datetime(df['Time'], unit='s').dt.hour
df['day_of_week'] = pd.to_datetime(df['Time'], unit='s').dt.dayofweek
df['amount_log'] = np.log1p(df['Amount'])

# Feature importance analysis
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)
print(feature_importance)
```

#### 3. Algorithm Selection
```python
# Try multiple algorithms
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

algorithms = {
    'RandomForest': RandomForestClassifier(),
    'XGBoost': XGBClassifier(),
    'LightGBM': LGBMClassifier()
}

results = {}
for name, model in algorithms.items():
    model.fit(X_train, y_train)
    accuracy = model.score(X_test, y_test)
    results[name] = accuracy
    print(f"{name}: {accuracy:.4f}")

# Output:
# RandomForest: 0.9234
# XGBoost: 0.9523  â† Winner!
# LightGBM: 0.9456
```

#### 4. Hyperparameter Tuning
```python
# Grid search for best hyperparameters
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.1, 0.2, 0.3],
    'n_estimators': [50, 100, 150, 200],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

xgb = XGBClassifier()
grid_search = GridSearchCV(
    xgb, 
    param_grid, 
    cv=5, 
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best accuracy:", grid_search.best_score_)

# Output:
# Best parameters: {
#     'max_depth': 5,
#     'learning_rate': 0.2,
#     'n_estimators': 100,
#     'subsample': 0.8,
#     'colsample_bytree': 0.8
# }
# Best accuracy: 0.9523
```

#### 5. SageMaker Training Experiments
```python
# Use SageMaker for scalable training
import sagemaker
from sagemaker.xgboost import XGBoost

# Define estimator
estimator = XGBoost(
    entry_point='train.py',
    role=sagemaker_role,
    instance_count=1,
    instance_type='ml.m5.xlarge',
    framework_version='1.5-1',
    hyperparameters={
        'objective': 'binary:logistic',
        'num_round': 100,
        'max_depth': 5,
        'eta': 0.2,
        'subsample': 0.8,
        'colsample_bytree': 0.8
    }
)

# Train on SageMaker
estimator.fit({
    'train': 's3://fraud-detection-data/prepared/train.parquet',
    'validation': 's3://fraud-detection-data/prepared/validation.parquet'
})

# Evaluate
predictor = estimator.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.large'
)

# Test predictions
test_predictions = predictor.predict(X_test)
accuracy = accuracy_score(y_test, test_predictions)
print(f"Test accuracy: {accuracy}")
```

#### 6. Model Comparison & Selection
```python
# Compare multiple trained models
experiments = [
    {'name': 'XGBoost-v1', 'accuracy': 0.9523, 'precision': 0.89, 'recall': 0.85},
    {'name': 'XGBoost-v2', 'accuracy': 0.9556, 'precision': 0.91, 'recall': 0.87},
    {'name': 'LightGBM-v1', 'accuracy': 0.9456, 'precision': 0.88, 'recall': 0.84},
]

# Select winner based on business requirements
# For fraud detection: Prioritize recall (catch all fraud)
winner = max(experiments, key=lambda x: x['recall'])
print(f"Selected model: {winner['name']}")
print(f"Configuration: max_depth=5, eta=0.2, num_round=100")
```

### Output of Experimentation Phase

**Deliverables:**
1. **Winning Algorithm**: XGBoost
2. **Best Hyperparameters**:
   ```python
   {
       'objective': 'binary:logistic',
       'num_round': 100,
       'max_depth': 5,
       'eta': 0.2,
       'subsample': 0.8,
       'colsample_bytree': 0.8
   }
   ```
3. **Expected Performance**: 95.23% accuracy, 89% precision, 85% recall
4. **Feature Set**: V1-V28, Time, Amount (from Kaggle dataset)
5. **Training Data Requirements**: 70/15/15 split, Parquet format

---

## Phase 2: Production Deployment (Automated Pipelines)

### Purpose
Take the winning configuration from experimentation and automate it for continuous, reliable production use.

### Implementation

#### 1. Codify Hyperparameters
```kotlin
// TrainHandler.kt
// These values come directly from notebook experiments
class TrainHandler : WorkflowLambdaHandler() {
    override fun processData(input: JsonNode): JsonNode {
        val trainingJobRequest = CreateTrainingJobRequest.builder()
            .hyperParameters(mapOf(
                "objective" to "binary:logistic",      // From experiments
                "num_round" to "100",                  // From experiments
                "max_depth" to "5",                    // From experiments
                "eta" to "0.2",                        // From experiments
                "subsample" to "0.8",                  // From experiments
                "colsample_bytree" to "0.8"           // From experiments
            ))
            .build()
        // ... rest of training logic
    }
}
```

#### 2. Add Validation Gates
```kotlin
// EvaluateHandler.kt
// Ensure model meets minimum quality threshold
if (accuracy < 0.90) {
    throw IllegalStateException(
        "Model accuracy $accuracy is below minimum threshold 0.90. " +
        "Training failed to produce acceptable model."
    )
}
```

#### 3. Schedule Automated Retraining
```yaml
# EventBridge Schedule
TrainingSchedule:
  Type: AWS::Events::Rule
  Properties:
    ScheduleExpression: "cron(0 2 ? * SUN *)"  # Weekly on Sunday 2 AM
    Targets:
      - Arn: !GetAtt TrainingStateMachine.Arn
```

### Benefits of Production Automation

| Aspect | Manual (Notebook) | Automated (Pipeline) |
|--------|------------------|---------------------|
| **Consistency** | Varies by data scientist | Identical every time |
| **Reliability** | Depends on human | Automated, monitored |
| **Scheduling** | Manual execution | Runs on schedule |
| **Validation** | Manual checks | Automated gates |
| **Monitoring** | Ad-hoc | Built-in alerts |
| **Rollback** | Manual | Automated |
| **Audit Trail** | Limited | Complete history |

---

## Phase 3: Monitoring & Drift Detection

### Purpose
Continuously monitor production models to detect when performance degrades or data patterns change.

### Key Metrics

#### 1. Model Performance Metrics
```kotlin
// MonitorHandler.kt
val metrics = calculateMetrics(scoredTransactions)

// Track over time
metrics = {
    "avgFraudScore": 0.4573,
    "highRiskPct": 0.05,      // 5% of transactions are high-risk
    "mediumRiskPct": 0.15,
    "lowRiskPct": 0.80,
    "accuracy": 0.9523,        // If ground truth available
    "precision": 0.89,
    "recall": 0.85
}
```

#### 2. Data Drift Detection
```kotlin
// Compare current distribution to baseline
val baseline = loadHistoricalBaseline()  // Last 30 days average

val avgScoreDrift = abs(currentAvgScore - baseline.avgFraudScore)
val highRiskDrift = abs(currentHighRiskPct - baseline.highRiskPct)

// Alert if significant drift
if (avgScoreDrift > 0.1 || highRiskDrift > 0.05) {
    sendAlert("Model drift detected! Avg score drift: $avgScoreDrift")
}
```

#### 3. Feature Drift Detection
```python
# In monitoring notebook (triggered by alert)
from scipy.stats import ks_2samp

# Compare feature distributions
for feature in features:
    baseline_dist = baseline_data[feature]
    current_dist = current_data[feature]
    
    # Kolmogorov-Smirnov test
    statistic, pvalue = ks_2samp(baseline_dist, current_dist)
    
    if pvalue < 0.05:
        print(f"Feature {feature} has drifted significantly!")
```

### Monitoring Dashboard Example

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fraud Detection Model - Production Monitoring          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Current Performance (Last 7 Days)                      â”‚
â”‚  â€¢ Avg Fraud Score: 0.457 (â†‘ 0.023 from baseline)      â”‚
â”‚  â€¢ High Risk %: 5.2% (â†‘ 0.8% from baseline)            â”‚
â”‚  â€¢ Accuracy: 94.1% (â†“ 1.2% from baseline) âš ï¸           â”‚
â”‚                                                          â”‚
â”‚  Alerts:                                                 â”‚
â”‚  ğŸš¨ Model drift detected on 2024-01-15                  â”‚
â”‚  âš ï¸  Accuracy below 95% threshold                       â”‚
â”‚                                                          â”‚
â”‚  Action Required:                                        â”‚
â”‚  â†’ Investigate in notebook                              â”‚
â”‚  â†’ Consider retraining with recent data                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 4: Continuous Improvement

### Purpose
When monitoring detects issues, data scientists return to notebooks to investigate and improve the model.

### Triggered By

1. **Performance Degradation**: Accuracy drops below threshold
2. **Data Drift**: Input distributions change significantly
3. **Concept Drift**: Relationship between features and target changes
4. **Business Changes**: New fraud patterns emerge
5. **Scheduled Reviews**: Quarterly improvement cycles

### Investigation Process

```python
# Triggered by monitoring alert
# Data scientist investigates in notebook

# 1. Load recent data
recent_data = load_data('2024-01-01', '2024-01-31')
baseline_data = load_data('2023-01-01', '2023-12-31')

# 2. Analyze what changed
print("Recent fraud rate:", recent_data['Class'].mean())
print("Baseline fraud rate:", baseline_data['Class'].mean())

# 3. Check feature distributions
for feature in features:
    plot_distribution_comparison(baseline_data[feature], recent_data[feature])

# 4. Identify new patterns
new_fraud_patterns = analyze_fraud_patterns(recent_data)
print("New patterns detected:", new_fraud_patterns)

# 5. Retrain with new data
model = XGBClassifier(**current_hyperparameters)
model.fit(recent_data[features], recent_data['Class'])
new_accuracy = model.score(test_data[features], test_data['Class'])

print(f"Current model: {current_accuracy}")
print(f"Retrained model: {new_accuracy}")

# 6. If improvement found, experiment with hyperparameters
if new_accuracy > current_accuracy:
    # Fine-tune for new data patterns
    param_grid = {
        'max_depth': [5, 7, 10],
        'eta': [0.15, 0.2, 0.25]
    }
    # ... grid search ...
```

### Update Production Pipeline

Once improvements are found:

```python
# Document new configuration
new_config = {
    'max_depth': 7,        # Changed from 5
    'eta': 0.15,           # Changed from 0.2
    'num_round': 150,      # Changed from 100
    'reason': 'Adapted to new fraud patterns detected in Jan 2024',
    'expected_accuracy': 0.961,
    'tested_on': '2024-01-31'
}

# Save to configuration management
save_config('production-hyperparameters-v2.yaml', new_config)
```

```kotlin
// Update TrainHandler.kt
hyperParameters = mapOf(
    "max_depth" to "7",      // Updated
    "eta" to "0.15",         // Updated
    "num_round" to "150"     // Updated
)
```

---

## Real Company Examples

### Netflix: Recommendation Systems

**Experimentation Phase:**
- Data scientists use Jupyter notebooks on internal ML platform
- Experiment with collaborative filtering, deep learning, hybrid models
- A/B test multiple models on small user segments
- Typical experimentation cycle: 2-4 weeks

**Production Phase:**
- Winning models deployed to production via automated pipelines
- Retraining happens daily with fresh user interaction data
- Models serve 200M+ users globally
- Automated rollback if performance degrades

**Monitoring & Improvement:**
- Track engagement metrics (watch time, completion rate)
- Quarterly model improvement cycles
- New features (e.g., time-of-day preferences) tested in notebooks first
- Production updates deployed via configuration management

**Tools:**
- Notebooks: Jupyter on internal platform
- Production: Scala/Java services, Spark for training
- Monitoring: Custom dashboards, A/B testing framework

### Uber: ETA Prediction

**Experimentation Phase:**
- Data scientists experiment with gradient boosting, neural networks
- Test different feature sets (traffic, weather, events)
- Hyperparameter tuning using Bayesian optimization
- Typical cycle: 1-2 weeks per experiment

**Production Phase:**
- Automated retraining every 6 hours with latest trip data
- Models deployed to edge locations for low latency
- Gradual rollout (1% â†’ 10% â†’ 50% â†’ 100%)
- Automated canary analysis

**Monitoring & Improvement:**
- Real-time accuracy tracking (predicted vs actual ETA)
- Alert if error exceeds 15% threshold
- Weekly review of model performance by city
- Monthly experimentation sprints for improvements

**Tools:**
- Notebooks: Jupyter, Databricks
- Production: Go services, Michelangelo ML platform
- Monitoring: Prometheus, Grafana, custom ML metrics

### Spotify: Music Recommendations

**Experimentation Phase:**
- Researchers experiment with collaborative filtering, NLP, audio analysis
- Test models on historical listening data
- Evaluate using offline metrics (precision@k, NDCG)
- Online A/B tests with 1% of users

**Production Phase:**
- Multiple models in production (Discover Weekly, Daily Mix, Radio)
- Retraining schedules vary by model (daily to weekly)
- Feature flags for gradual rollout
- Automated quality checks before deployment

**Monitoring & Improvement:**
- Track user engagement (streams, saves, skips)
- Quarterly "hack weeks" for experimentation
- New models tested in parallel with production
- Champion/challenger framework for model updates

**Tools:**
- Notebooks: Jupyter, internal ML platform
- Production: Scala/Python services, Luigi pipelines
- Monitoring: Custom dashboards, A/B testing platform

### Amazon: Product Recommendations

**Experimentation Phase:**
- Scientists experiment with item-to-item collaborative filtering
- Test deep learning models for personalization
- Simulate on historical purchase data
- A/B test on small customer segments

**Production Phase:**
- Real-time recommendations for millions of products
- Continuous retraining with purchase/browse data
- Multi-armed bandit for exploration/exploitation
- Automated deployment with safety checks

**Monitoring & Improvement:**
- Track conversion rate, revenue per recommendation
- Daily performance reviews by category
- Seasonal adjustments (holidays, events)
- Continuous experimentation culture

**Tools:**
- Notebooks: SageMaker Studio, internal tools
- Production: Java services, SageMaker for training
- Monitoring: CloudWatch, custom ML metrics

---

## Integration Patterns

### Pattern 1: Configuration-Based Updates

**Notebook Output:**
```yaml
# experiments/fraud-detection-v2.yaml
model:
  algorithm: xgboost
  version: 2.0
  hyperparameters:
    max_depth: 7
    eta: 0.15
    num_round: 150
  performance:
    accuracy: 0.961
    precision: 0.92
    recall: 0.88
  tested_date: 2024-01-31
  approved_by: data-science-team
```

**Production Pipeline:**
```kotlin
// TrainHandler reads from config
val config = loadConfig("production-model-config.yaml")
hyperParameters = config.model.hyperparameters.toMap()
```

### Pattern 2: Parameter Store Integration

**Notebook Updates Parameter Store:**
```python
# After successful experiments
import boto3

ssm = boto3.client('ssm')

# Update hyperparameters
ssm.put_parameter(
    Name='/fraud-detection/hyperparameters/max_depth',
    Value='7',
    Type='String',
    Overwrite=True
)

ssm.put_parameter(
    Name='/fraud-detection/hyperparameters/eta',
    Value='0.15',
    Type='String',
    Overwrite=True
)
```

**Production Pipeline Reads:**
```kotlin
// TrainHandler reads from Parameter Store
val ssmClient = SsmClient.builder().build()

val maxDepth = ssmClient.getParameter(
    GetParameterRequest.builder()
        .name("/fraud-detection/hyperparameters/max_depth")
        .build()
).parameter().value()

val eta = ssmClient.getParameter(
    GetParameterRequest.builder()
        .name("/fraud-detection/hyperparameters/eta")
        .build()
).parameter().value()

hyperParameters = mapOf(
    "max_depth" to maxDepth,
    "eta" to eta,
    // ... other parameters
)
```

### Pattern 3: Model Registry

**Notebook Registers Model:**
```python
# After training and validation
import mlflow

with mlflow.start_run():
    # Train model
    model = train_xgboost(hyperparameters)
    
    # Log metrics
    mlflow.log_metric("accuracy", 0.961)
    mlflow.log_metric("precision", 0.92)
    
    # Log hyperparameters
    mlflow.log_params(hyperparameters)
    
    # Register model
    mlflow.sklearn.log_model(
        model,
        "fraud-detection-model",
        registered_model_name="FraudDetection"
    )
    
    # Promote to production
    client = mlflow.tracking.MlflowClient()
    client.transition_model_version_stage(
        name="FraudDetection",
        version=2,
        stage="Production"
    )
```

**Production Pipeline Uses Registry:**
```kotlin
// TrainHandler checks model registry for latest approved config
val modelRegistry = ModelRegistryClient()
val productionModel = modelRegistry.getLatestVersion(
    modelName = "FraudDetection",
    stage = "Production"
)

// Use hyperparameters from registered model
hyperParameters = productionModel.hyperparameters
```

---

## Summary

### Key Takeaways

1. **Experimentation and Production are Complementary**
   - Notebooks for discovery and improvement
   - Pipelines for automation and reliability
   - Both are essential for successful ML systems

2. **The Cycle Never Ends**
   - Experiment â†’ Deploy â†’ Monitor â†’ Improve â†’ Repeat
   - Continuous improvement is built into the process
   - Monitoring triggers re-experimentation

3. **Real Companies Use This Pattern**
   - Netflix, Uber, Spotify, Amazon all follow similar patterns
   - Scale varies, but principles remain the same
   - Automation frees data scientists to focus on improvement

4. **Integration is Key**
   - Configuration management bridges notebooks and production
   - Parameter stores, model registries enable seamless updates
   - Version control and audit trails ensure reproducibility

### Next Steps

For the fraud detection system:
1. âœ… Production pipeline implemented (current spec)
2. ğŸ”„ Experimentation workflow needed (new spec)
3. ğŸ”„ Integration patterns to connect them (new spec)

This ensures we have both the automated production system AND the experimentation tools needed for continuous improvement.
