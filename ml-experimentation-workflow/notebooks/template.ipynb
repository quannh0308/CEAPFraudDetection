{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deps-header",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "Run this cell once to ensure all required packages are available in the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deps-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once per kernel)\n",
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n",
    "    'boto3', 's3fs', 'pandas', 'numpy', 'matplotlib', 'seaborn',\n",
    "    'scikit-learn', 'xgboost', 'lightgbm', 'sagemaker', 'pyyaml'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Experiment Template\n",
    "\n",
    "This template provides pre-configured experiment tracking, production integration\n",
    "helpers, and automatic logging of code versions and dependencies.\n",
    "\n",
    "## How to use\n",
    "1. Copy this notebook and rename it for your experiment.\n",
    "2. Fill in the experiment configuration in the Setup cell.\n",
    "3. Add your training and evaluation code in the Experiment section.\n",
    "4. Use the Production Promotion section when ready to deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import platform\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# ── Experiment configuration ──────────────────────────────────────\n",
    "EXPERIMENT_NAME = 'fraud-detection-experiment'  # Change this\n",
    "ALGORITHM = 'xgboost'                           # e.g. xgboost, lightgbm, random_forest\n",
    "APPROVER = 'data-science-team'                  # Your name or team\n",
    "DATASET_VERSION = 'v1.0.0'                      # Version of the dataset\n",
    "\n",
    "print(f'Experiment: {EXPERIMENT_NAME}')\n",
    "print(f'Algorithm:  {ALGORITHM}')\n",
    "print(f'Started:    {datetime.now().isoformat()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automatic Code Version and Dependency Logging\n",
    "\n",
    "Captures the current git commit, Python version, and installed package versions\n",
    "for full reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_version():\n",
    "    \"\"\"Get the current git commit hash and branch.\"\"\"\n",
    "    try:\n",
    "        commit = subprocess.check_output(\n",
    "            ['git', 'rev-parse', 'HEAD'], stderr=subprocess.DEVNULL\n",
    "        ).decode().strip()\n",
    "    except Exception:\n",
    "        commit = 'unknown'\n",
    "    try:\n",
    "        branch = subprocess.check_output(\n",
    "            ['git', 'rev-parse', '--abbrev-ref', 'HEAD'], stderr=subprocess.DEVNULL\n",
    "        ).decode().strip()\n",
    "    except Exception:\n",
    "        branch = 'unknown'\n",
    "    try:\n",
    "        dirty = subprocess.check_output(\n",
    "            ['git', 'status', '--porcelain'], stderr=subprocess.DEVNULL\n",
    "        ).decode().strip()\n",
    "        is_dirty = len(dirty) > 0\n",
    "    except Exception:\n",
    "        is_dirty = False\n",
    "    return {\n",
    "        'git_commit': commit,\n",
    "        'git_branch': branch,\n",
    "        'git_dirty': is_dirty,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_dependency_versions():\n",
    "    \"\"\"Get versions of key ML dependencies.\"\"\"\n",
    "    packages = [\n",
    "        'boto3', 'sagemaker', 'pandas', 'numpy', 'scikit-learn',\n",
    "        'xgboost', 'lightgbm', 'matplotlib', 'seaborn', 'pyyaml',\n",
    "    ]\n",
    "    versions = {}\n",
    "    for pkg in packages:\n",
    "        try:\n",
    "            mod = importlib.import_module(pkg.replace('-', '_').replace('scikit-learn', 'sklearn'))\n",
    "            versions[pkg] = getattr(mod, '__version__', 'installed')\n",
    "        except ImportError:\n",
    "            versions[pkg] = 'not installed'\n",
    "    versions['python'] = platform.python_version()\n",
    "    return versions\n",
    "\n",
    "\n",
    "code_version_info = get_code_version()\n",
    "dependency_versions = get_dependency_versions()\n",
    "\n",
    "print('Code Version:')\n",
    "for k, v in code_version_info.items():\n",
    "    print(f'  {k}: {v}')\n",
    "\n",
    "print('\\nDependency Versions:')\n",
    "for k, v in dependency_versions.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiment_tracking import ExperimentTracker\n",
    "\n",
    "tracker = ExperimentTracker()\n",
    "\n",
    "# Start experiment with automatic metadata\n",
    "experiment_id = tracker.start_experiment(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    algorithm=ALGORITHM,\n",
    "    dataset_version=DATASET_VERSION,\n",
    "    code_version=code_version_info['git_commit'],\n",
    ")\n",
    "\n",
    "# Log code version and dependency info as parameters\n",
    "tracker.log_parameters(experiment_id, {\n",
    "    'code_git_commit': code_version_info['git_commit'],\n",
    "    'code_git_branch': code_version_info['git_branch'],\n",
    "    'code_git_dirty': str(code_version_info['git_dirty']),\n",
    "    'python_version': dependency_versions['python'],\n",
    "    'dataset_version': DATASET_VERSION,\n",
    "})\n",
    "\n",
    "print(f'Experiment ID: {experiment_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data from S3 (update paths for your dataset)\n",
    "# import os\n",
    "# BUCKET_SUFFIX = os.environ.get('BUCKET_SUFFIX', 'quannh0308-20260222')\n",
    "# BUCKET_NAME = f'fraud-detection-data-{BUCKET_SUFFIX}'\n",
    "# train_df = pd.read_parquet(f's3://{BUCKET_NAME}/prepared/train.parquet')\n",
    "# val_df = pd.read_parquet(f's3://{BUCKET_NAME}/prepared/validation.parquet')\n",
    "# test_df = pd.read_parquet(f's3://{BUCKET_NAME}/prepared/test.parquet')\n",
    "\n",
    "# Placeholder: generate sample data for template demonstration\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X = pd.DataFrame(X, columns=[f'V{i}' for i in range(20)])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training set:   {X_train.shape}')\n",
    "print(f'Test set:       {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model\n",
    "\n",
    "Replace this section with your training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'logloss',\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "# Log hyperparameters\n",
    "tracker.log_parameters(experiment_id, hyperparameters)\n",
    "\n",
    "# Train\n",
    "model = XGBClassifier(**hyperparameters)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_evaluation import ModelEvaluator\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Optional: define production baseline for comparison\n",
    "baseline_metrics = {\n",
    "    'accuracy': 0.952,\n",
    "    'precision': 0.89,\n",
    "    'recall': 0.85,\n",
    "    'f1_score': 0.87,\n",
    "    'auc_roc': 0.94,\n",
    "}\n",
    "\n",
    "# Full evaluation\n",
    "results = evaluator.evaluate_model(model, X_test, y_test, baseline_metrics)\n",
    "\n",
    "# Log metrics to tracker\n",
    "tracker.log_metrics(experiment_id, results['metrics'])\n",
    "\n",
    "print('Metrics:')\n",
    "for metric, value in results['metrics'].items():\n",
    "    print(f'  {metric}: {value:.4f}')\n",
    "\n",
    "print(f\"\\nMeets production threshold (>= 0.90): {results['meets_production_threshold']}\")\n",
    "\n",
    "if results['comparison']:\n",
    "    print('\\nComparison to baseline:')\n",
    "    for metric, comp in results['comparison'].items():\n",
    "        arrow = '↑' if comp['improved'] else '↓'\n",
    "        print(f\"  {metric}: {comp['current']:.4f} (baseline: {comp['baseline']:.4f}, \"\n",
    "              f\"{comp['percent_change']:+.2f}% {arrow})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Promotion Helpers\n",
    "\n",
    "Use these helpers when your model meets the production threshold and you want\n",
    "to promote the configuration to the production pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.production_integration import ProductionIntegrator\n",
    "\n",
    "\n",
    "def promote_experiment(experiment_id, hyperparameters, metrics, approver,\n",
    "                       trigger_pipeline=False):\n",
    "    \"\"\"Helper to promote an experiment to production.\n",
    "\n",
    "    Validates hyperparameters, backs up current Parameter Store values,\n",
    "    writes new values, generates S3 config, and optionally triggers\n",
    "    the production pipeline.\n",
    "\n",
    "    Args:\n",
    "        experiment_id: Unique experiment identifier.\n",
    "        hyperparameters: Dict with keys: objective, num_round, max_depth,\n",
    "                         eta, subsample, colsample_bytree.\n",
    "        metrics: Dict of performance metrics (accuracy, precision, etc.).\n",
    "        approver: Name of the person approving the promotion.\n",
    "        trigger_pipeline: Whether to trigger the production pipeline.\n",
    "\n",
    "    Returns:\n",
    "        Promotion result dict with promotion_event and execution_arn.\n",
    "    \"\"\"\n",
    "    integrator = ProductionIntegrator(experiment_tracker=tracker)\n",
    "    return integrator.promote_to_production(\n",
    "        experiment_id=experiment_id,\n",
    "        hyperparameters=hyperparameters,\n",
    "        metrics=metrics,\n",
    "        approver=approver,\n",
    "        trigger_pipeline=trigger_pipeline,\n",
    "    )\n",
    "\n",
    "\n",
    "def check_promotion_readiness(metrics, threshold=0.90):\n",
    "    \"\"\"Check if metrics meet the production promotion threshold.\n",
    "\n",
    "    Args:\n",
    "        metrics: Dict of performance metrics.\n",
    "        threshold: Minimum accuracy required (default 0.90).\n",
    "\n",
    "    Returns:\n",
    "        True if accuracy meets the threshold.\n",
    "    \"\"\"\n",
    "    accuracy = metrics.get('accuracy', 0)\n",
    "    ready = accuracy >= threshold\n",
    "    if ready:\n",
    "        print(f'✓ Model is production-ready (accuracy={accuracy:.4f} >= {threshold})')\n",
    "    else:\n",
    "        print(f'✗ Model does not meet threshold (accuracy={accuracy:.4f} < {threshold})')\n",
    "    return ready\n",
    "\n",
    "\n",
    "print('Production promotion helpers loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Promote to production (uncomment when ready) ─────────────────\n",
    "\n",
    "# production_params = {\n",
    "#     'objective': 'binary:logistic',\n",
    "#     'num_round': hyperparameters['n_estimators'],\n",
    "#     'max_depth': hyperparameters['max_depth'],\n",
    "#     'eta': hyperparameters['learning_rate'],\n",
    "#     'subsample': hyperparameters['subsample'],\n",
    "#     'colsample_bytree': hyperparameters['colsample_bytree'],\n",
    "# }\n",
    "#\n",
    "# if check_promotion_readiness(results['metrics']):\n",
    "#     result = promote_experiment(\n",
    "#         experiment_id=experiment_id,\n",
    "#         hyperparameters=production_params,\n",
    "#         metrics=results['metrics'],\n",
    "#         approver=APPROVER,\n",
    "#         trigger_pipeline=True,\n",
    "#     )\n",
    "#     print(f\"Promoted! Backup: {result['promotion_event']['backup_key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the experiment run\n",
    "tracker.close_experiment(experiment_id)\n",
    "print(f'Experiment {experiment_id} closed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
