{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Data Exploration - Fraud Detection Dataset\n",
    "\n",
    "This notebook demonstrates loading and exploring the fraud detection dataset from S3.\n",
    "We analyze feature distributions, dataset statistics, and class imbalance to inform\n",
    "feature engineering and model training decisions.\n",
    "\n",
    "**Requirements covered:** 2.1 (Load Parquet from S3), 2.2 (Visualization utilities), 2.3 (Statistical summaries), 2.4 (Schema and sample records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project src to path for ExperimentTracker\n",
    "sys.path.insert(0, '../src')\n",
    "from experiment_tracking import ExperimentTracker\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## 2. Load Data from S3\n",
    "\n",
    "Load the train, validation, and test Parquet splits from the `fraud-detection-data` S3 bucket.\n",
    "\n",
    "**Requirement 2.1**: Load Parquet datasets from S3 (train, validation, test splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'fraud-detection-data'\n",
    "DATA_PREFIX = 'processed'\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def load_parquet_from_s3(bucket: str, key: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a Parquet file from S3 into a pandas DataFrame.\"\"\"\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_parquet(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "\n",
    "train_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/train.parquet')\n",
    "val_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/validation.parquet')\n",
    "test_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/test.parquet')\n",
    "\n",
    "print(f'Train set:      {train_df.shape[0]:>8,} rows, {train_df.shape[1]} columns')\n",
    "print(f'Validation set: {val_df.shape[0]:>8,} rows, {val_df.shape[1]} columns')\n",
    "print(f'Test set:       {test_df.shape[0]:>8,} rows, {test_df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "## 3. Dataset Schema and Sample Records\n",
    "\n",
    "**Requirement 2.4**: Display dataset schema and sample records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Dataset Schema ===')\n",
    "print(f'{\"Column\":<20} {\"Dtype\":<15} {\"Non-Null Count\"}')\n",
    "print('-' * 55)\n",
    "for col in train_df.columns:\n",
    "    non_null = train_df[col].notna().sum()\n",
    "    print(f'{col:<20} {str(train_df[col].dtype):<15} {non_null}/{len(train_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Sample Records (first 5 rows) ===')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## 4. Statistical Summary\n",
    "\n",
    "**Requirement 2.3**: Statistical summary functions for dataset characteristics (record counts, missing values, feature ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_summary(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Generate a statistical summary for a dataset split.\n",
    "\n",
    "    Returns a DataFrame with count, missing, min, max, mean, std, and\n",
    "    median for every column.\n",
    "    \"\"\"\n",
    "    summary = pd.DataFrame({\n",
    "        'count': df.count(),\n",
    "        'missing': df.isnull().sum(),\n",
    "        'missing_pct': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "        'min': df.min(numeric_only=True),\n",
    "        'max': df.max(numeric_only=True),\n",
    "        'mean': df.mean(numeric_only=True).round(4),\n",
    "        'std': df.std(numeric_only=True).round(4),\n",
    "        'median': df.median(numeric_only=True).round(4)\n",
    "    })\n",
    "    print(f'\\n=== {name} Summary ({len(df):,} records) ===')\n",
    "    return summary\n",
    "\n",
    "\n",
    "train_summary = dataset_summary(train_df, 'Train')\n",
    "train_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_summary = dataset_summary(val_df, 'Validation')\n",
    "val_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary = dataset_summary(test_df, 'Test')\n",
    "test_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## 5. Feature Distribution Visualizations\n",
    "\n",
    "**Requirement 2.2**: Visualization utilities for feature distributions, correlations, and class imbalance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distributions(df: pd.DataFrame, features: list, ncols: int = 4) -> None:\n",
    "    \"\"\"Plot histograms for the given features, coloured by Class label.\"\"\"\n",
    "    nrows = int(np.ceil(len(features) / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feat in enumerate(features):\n",
    "        ax = axes[idx]\n",
    "        for label, colour in [(0, 'steelblue'), (1, 'crimson')]:\n",
    "            subset = df[df['Class'] == label][feat]\n",
    "            ax.hist(subset, bins=50, alpha=0.6, label=f'Class {label}', color=colour)\n",
    "        ax.set_title(feat)\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    # Hide unused axes\n",
    "    for idx in range(len(features), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot PCA features V1-V28\n",
    "pca_features = [f'V{i}' for i in range(1, 29)]\n",
    "plot_feature_distributions(train_df, pca_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Time and Amount\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(train_df['Time'], bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0].set_title('Transaction Time Distribution')\n",
    "axes[0].set_xlabel('Time (seconds)')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "axes[1].hist(train_df['Amount'], bins=50, color='darkorange', edgecolor='white')\n",
    "axes[1].set_title('Transaction Amount Distribution')\n",
    "axes[1].set_xlabel('Amount')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": [
    "### 5.1 Feature Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df: pd.DataFrame, title: str = 'Feature Correlation Matrix') -> None:\n",
    "    \"\"\"Plot a correlation heatmap for all numeric features.\"\"\"\n",
    "    corr = df.select_dtypes(include=[np.number]).corr()\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    sns.heatmap(corr, mask=mask, cmap='coolwarm', center=0,\n",
    "                linewidths=0.5, fmt='.2f', square=True)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_correlation_heatmap(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1",
   "metadata": {},
   "source": [
    "### 5.2 Top Correlated Features with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_corr = train_df.select_dtypes(include=[np.number]).corr()['Class'].drop('Class').abs().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "target_corr.head(15).plot(kind='barh', color='teal')\n",
    "plt.title('Top 15 Features Correlated with Class (absolute value)')\n",
    "plt.xlabel('|Correlation|')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d2e3",
   "metadata": {},
   "source": [
    "## 6. Class Imbalance Analysis\n",
    "\n",
    "Fraud detection datasets are typically highly imbalanced. Understanding the class\n",
    "distribution is critical for choosing the right evaluation metrics and sampling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_class_imbalance(df: pd.DataFrame, name: str) -> dict:\n",
    "    \"\"\"Analyze and visualize class distribution.\n",
    "\n",
    "    Returns a dict with counts, percentages, and imbalance ratio.\n",
    "    \"\"\"\n",
    "    counts = df['Class'].value_counts().sort_index()\n",
    "    total = len(df)\n",
    "    legit = int(counts.get(0, 0))\n",
    "    fraud = int(counts.get(1, 0))\n",
    "    ratio = legit / fraud if fraud > 0 else float('inf')\n",
    "\n",
    "    print(f'=== {name} Class Distribution ===')\n",
    "    print(f'  Legitimate (0): {legit:>8,}  ({legit/total*100:.2f}%)')\n",
    "    print(f'  Fraud      (1): {fraud:>8,}  ({fraud/total*100:.2f}%)')\n",
    "    print(f'  Imbalance ratio: {ratio:.1f}:1')\n",
    "\n",
    "    return {'legitimate': legit, 'fraud': fraud, 'ratio': ratio}\n",
    "\n",
    "\n",
    "train_imbalance = analyze_class_imbalance(train_df, 'Train')\n",
    "val_imbalance = analyze_class_imbalance(val_df, 'Validation')\n",
    "test_imbalance = analyze_class_imbalance(test_df, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution across splits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, (df, name) in zip(axes, [(train_df, 'Train'), (val_df, 'Validation'), (test_df, 'Test')]):\n",
    "    counts = df['Class'].value_counts().sort_index()\n",
    "    colors = ['steelblue', 'crimson']\n",
    "    bars = ax.bar(['Legitimate (0)', 'Fraud (1)'], counts.values, color=colors)\n",
    "    ax.set_title(f'{name} Set Class Distribution')\n",
    "    ax.set_ylabel('Count')\n",
    "    for bar, count in zip(bars, counts.values):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
    "                f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount distribution by class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, label, title in zip(axes, [0, 1], ['Legitimate Transactions', 'Fraudulent Transactions']):\n",
    "    subset = train_df[train_df['Class'] == label]['Amount']\n",
    "    ax.hist(subset, bins=50, color='steelblue' if label == 0 else 'crimson', edgecolor='white')\n",
    "    ax.set_title(f'{title} - Amount Distribution')\n",
    "    ax.set_xlabel('Amount')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b6c7",
   "metadata": {},
   "source": [
    "## 7. Log Exploration to ExperimentTracker\n",
    "\n",
    "Record the data exploration session so it is tracked alongside model experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = ExperimentTracker(region_name='us-east-1')\n",
    "\n",
    "experiment_id = tracker.start_experiment(\n",
    "    experiment_name='fraud-detection-data-exploration',\n",
    "    algorithm='data-exploration',\n",
    "    user='data-scientist',\n",
    "    dataset_version='v1.0',\n",
    "    code_version='notebook-01'\n",
    ")\n",
    "\n",
    "tracker.log_parameters(experiment_id, {\n",
    "    'train_rows': int(train_df.shape[0]),\n",
    "    'val_rows': int(val_df.shape[0]),\n",
    "    'test_rows': int(test_df.shape[0]),\n",
    "    'num_features': int(train_df.shape[1]),\n",
    "    'bucket': BUCKET_NAME\n",
    "})\n",
    "\n",
    "tracker.log_metrics(experiment_id, {\n",
    "    'train_fraud_ratio': train_imbalance['ratio'],\n",
    "    'train_fraud_count': float(train_imbalance['fraud']),\n",
    "    'train_legit_count': float(train_imbalance['legitimate']),\n",
    "    'missing_values_total': float(train_df.isnull().sum().sum())\n",
    "})\n",
    "\n",
    "tracker.close_experiment(experiment_id)\n",
    "\n",
    "print(f'Exploration logged as experiment: {experiment_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key observations from data exploration:\n",
    "\n",
    "1. **Schema**: The dataset contains Time, Amount, V1-V28 (PCA features), and Class columns.\n",
    "2. **Class imbalance**: Fraud cases are a small minority â€” consider oversampling, SMOTE, or class-weight adjustments during training.\n",
    "3. **Feature correlations**: Several V-features show meaningful correlation with the target, which can guide feature selection.\n",
    "4. **Amount distribution**: Fraudulent transactions tend to have different amount patterns than legitimate ones.\n",
    "\n",
    "Next steps: proceed to feature engineering (notebook 04) or hyperparameter tuning (notebook 02)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}