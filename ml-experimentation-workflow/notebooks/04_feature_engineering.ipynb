{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# Feature Engineering — Fraud Detection\n",
    "\n",
    "This notebook demonstrates feature engineering techniques for the fraud detection\n",
    "dataset using the project's `FeatureEngineer` class and `ExperimentTracker`.\n",
    "\n",
    "We cover:\n",
    "1. **Time feature creation** — extract hour, day-of-week, weekend, and night indicators\n",
    "2. **Amount feature creation** — log, squared, and square-root transformations\n",
    "3. **Interaction feature creation** — pairwise products of selected features\n",
    "4. **Feature selection** — univariate (SelectKBest) and RFE methods\n",
    "5. **Feature importance analysis** — Random Forest importance ranking\n",
    "6. **Impact analysis** — compare model performance with selected vs all features\n",
    "\n",
    "All experiments are logged to the `ExperimentTracker` for reproducibility.\n",
    "\n",
    "**Requirements covered:** 5.1 (derived features), 5.2 (feature selection), 5.3 (experiment logging), 5.4 (feature impact analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000002",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Add project src to path\n",
    "sys.path.insert(0, '../src')\n",
    "from feature_engineering import FeatureEngineer\n",
    "from experiment_tracking import ExperimentTracker\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000004",
   "metadata": {},
   "source": [
    "## 2. Load Data from S3\n",
    "\n",
    "Load the processed fraud detection dataset from the `fraud-detection-data` bucket.\n",
    "The dataset contains columns `Time`, `Amount`, `V1`–`V28`, and `Class` (0 = legitimate, 1 = fraud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'fraud-detection-data'\n",
    "DATA_PREFIX = 'processed'\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def load_parquet_from_s3(bucket: str, key: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a Parquet file from S3 into a pandas DataFrame.\"\"\"\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_parquet(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "\n",
    "train_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/train.parquet')\n",
    "test_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/test.parquet')\n",
    "\n",
    "print(f'Training set:  {train_df.shape[0]:,} rows, {train_df.shape[1]} columns')\n",
    "print(f'Test set:      {test_df.shape[0]:,} rows, {test_df.shape[1]} columns')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000006",
   "metadata": {},
   "source": [
    "## 3. Time Feature Creation\n",
    "\n",
    "The `Time` column represents seconds since the first transaction in the dataset.\n",
    "`create_time_features` converts it to human-readable temporal signals:\n",
    "- `hour` — hour of the day (0–23)\n",
    "- `day_of_week` — day of the week (0 = Monday, 6 = Sunday)\n",
    "- `is_weekend` — 1 if Saturday or Sunday, 0 otherwise\n",
    "- `is_night` — 1 if hour is between 22:00 and 06:00\n",
    "\n",
    "**Requirement 5.1**: Provide utilities for creating derived features (time-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineer = FeatureEngineer()\n",
    "\n",
    "# Create time features on training data\n",
    "train_df = engineer.create_time_features(train_df)\n",
    "test_df = engineer.create_time_features(test_df)\n",
    "\n",
    "time_features = ['hour', 'day_of_week', 'is_weekend', 'is_night']\n",
    "print('New time features:')\n",
    "train_df[time_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time feature distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Hour distribution by class\n",
    "ax = axes[0, 0]\n",
    "for label, group in train_df.groupby('Class'):\n",
    "    ax.hist(group['hour'], bins=24, alpha=0.6,\n",
    "            label='Legitimate' if label == 0 else 'Fraud', density=True)\n",
    "ax.set_xlabel('Hour of Day')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Transaction Hour Distribution by Class')\n",
    "ax.legend()\n",
    "\n",
    "# Day of week distribution by class\n",
    "ax = axes[0, 1]\n",
    "day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "for label, group in train_df.groupby('Class'):\n",
    "    counts = group['day_of_week'].value_counts().sort_index()\n",
    "    ax.bar(counts.index + (-0.2 if label == 0 else 0.2), counts.values / counts.sum(),\n",
    "           width=0.4, alpha=0.7, label='Legitimate' if label == 0 else 'Fraud')\n",
    "ax.set_xticks(range(7))\n",
    "ax.set_xticklabels(day_labels)\n",
    "ax.set_xlabel('Day of Week')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Day of Week Distribution by Class')\n",
    "ax.legend()\n",
    "\n",
    "# Weekend vs weekday fraud rate\n",
    "ax = axes[1, 0]\n",
    "weekend_fraud = train_df.groupby('is_weekend')['Class'].mean()\n",
    "bars = ax.bar(['Weekday', 'Weekend'], weekend_fraud.values, color=['steelblue', 'coral'])\n",
    "for bar, val in zip(bars, weekend_fraud.values):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.0002,\n",
    "            f'{val:.4f}', ha='center')\n",
    "ax.set_ylabel('Fraud Rate')\n",
    "ax.set_title('Fraud Rate: Weekday vs Weekend')\n",
    "\n",
    "# Night vs day fraud rate\n",
    "ax = axes[1, 1]\n",
    "night_fraud = train_df.groupby('is_night')['Class'].mean()\n",
    "bars = ax.bar(['Daytime', 'Nighttime'], night_fraud.values, color=['steelblue', 'coral'])\n",
    "for bar, val in zip(bars, night_fraud.values):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.0002,\n",
    "            f'{val:.4f}', ha='center')\n",
    "ax.set_ylabel('Fraud Rate')\n",
    "ax.set_title('Fraud Rate: Daytime vs Nighttime')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('time_features.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000009",
   "metadata": {},
   "source": [
    "## 4. Amount Feature Creation\n",
    "\n",
    "`create_amount_features` derives three transformations of the `Amount` column:\n",
    "- `amount_log` — log(1 + Amount), reduces skewness\n",
    "- `amount_squared` — Amount², amplifies large-value differences\n",
    "- `amount_sqrt` — √Amount, compresses large values\n",
    "\n",
    "**Requirement 5.1**: Provide utilities for creating derived features (aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = engineer.create_amount_features(train_df)\n",
    "test_df = engineer.create_amount_features(test_df)\n",
    "\n",
    "amount_features = ['Amount', 'amount_log', 'amount_squared', 'amount_sqrt']\n",
    "print('Amount feature statistics:')\n",
    "train_df[amount_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize amount feature distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, feat in enumerate(amount_features):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    for label, group in train_df.groupby('Class'):\n",
    "        ax.hist(group[feat], bins=50, alpha=0.6, density=True,\n",
    "                label='Legitimate' if label == 0 else 'Fraud')\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{feat} Distribution by Class')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('amount_features.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000012",
   "metadata": {},
   "source": [
    "## 5. Interaction Feature Creation\n",
    "\n",
    "`create_interaction_features` computes element-wise products for specified\n",
    "feature pairs. This captures non-linear relationships between features.\n",
    "\n",
    "**Requirement 5.1**: Provide utilities for creating derived features (interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000013",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_pairs = [('V1', 'V2'), ('V3', 'V4'), ('V1', 'Amount')]\n",
    "\n",
    "train_df = engineer.create_interaction_features(train_df, interaction_pairs)\n",
    "test_df = engineer.create_interaction_features(test_df, interaction_pairs)\n",
    "\n",
    "interaction_cols = [f'{a}_x_{b}' for a, b in interaction_pairs]\n",
    "print('Interaction features created:', interaction_cols)\n",
    "train_df[interaction_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000014",
   "metadata": {},
   "source": [
    "## 6. Feature Selection — Univariate (SelectKBest)\n",
    "\n",
    "`select_features_univariate` uses the ANOVA F-value (`f_classif`) to rank\n",
    "features by their individual predictive power and selects the top *k*.\n",
    "\n",
    "**Requirement 5.2**: Provide feature selection methods (feature importance, correlation analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix (exclude target and raw Time)\n",
    "TARGET = 'Class'\n",
    "EXCLUDE = [TARGET, 'Time']\n",
    "ALL_FEATURES = [c for c in train_df.columns if c not in EXCLUDE]\n",
    "\n",
    "X_train = train_df[ALL_FEATURES]\n",
    "y_train = train_df[TARGET]\n",
    "X_test = test_df[ALL_FEATURES]\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "print(f'Total features available: {len(ALL_FEATURES)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000016",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20\n",
    "selected_univariate, scores_df = engineer.select_features_univariate(X_train, y_train, k=K)\n",
    "\n",
    "print(f'Top {K} features (univariate):')\n",
    "print(selected_univariate)\n",
    "scores_df.head(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize univariate feature scores\n",
    "top_scores = scores_df.head(K)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=top_scores, x='score', y='feature', palette='viridis')\n",
    "plt.title(f'Top {K} Features — Univariate F-Score (SelectKBest)')\n",
    "plt.xlabel('F-Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.savefig('univariate_feature_scores.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000018",
   "metadata": {},
   "source": [
    "## 7. Feature Selection — Recursive Feature Elimination (RFE)\n",
    "\n",
    "`select_features_rfe` uses a Random Forest estimator to iteratively remove the\n",
    "least important features until the desired count is reached. Features with\n",
    "ranking = 1 are selected.\n",
    "\n",
    "**Requirement 5.2**: Provide feature selection methods (recursive elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES_RFE = 20\n",
    "selected_rfe, ranking_df = engineer.select_features_rfe(X_train, y_train, n_features=N_FEATURES_RFE)\n",
    "\n",
    "print(f'Top {N_FEATURES_RFE} features (RFE):')\n",
    "print(selected_rfe)\n",
    "ranking_df.head(N_FEATURES_RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RFE rankings\n",
    "plt.figure(figsize=(12, 8))\n",
    "sorted_ranking = ranking_df.sort_values('ranking')\n",
    "colors = ['forestgreen' if r == 1 else 'lightgray' for r in sorted_ranking['ranking']]\n",
    "sns.barplot(data=sorted_ranking, x='ranking', y='feature', palette=colors)\n",
    "plt.title('Feature Rankings — Recursive Feature Elimination')\n",
    "plt.xlabel('Ranking (1 = selected)')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rfe_feature_rankings.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000021",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis\n",
    "\n",
    "`analyze_feature_importance` trains a Random Forest on the full feature set and\n",
    "returns Gini importance scores. This gives a holistic view of which features\n",
    "contribute most to the model's decisions.\n",
    "\n",
    "**Requirement 5.2**: Provide feature selection methods (feature importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000022",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = engineer.analyze_feature_importance(X_train, y_train)\n",
    "\n",
    "print('Top 20 features by Random Forest importance:')\n",
    "importance_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of feature importance\n",
    "top_importance = importance_df.head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=top_importance, x='importance', y='feature', palette='magma')\n",
    "plt.title('Top 20 Features — Random Forest Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000024",
   "metadata": {},
   "source": [
    "## 9. Feature Impact Analysis — Selected vs All Features\n",
    "\n",
    "To quantify the impact of feature selection, we train an XGBoost model twice:\n",
    "1. Using **all** available features\n",
    "2. Using only the **selected** features from univariate selection\n",
    "\n",
    "This shows whether feature selection improves or maintains performance while\n",
    "reducing dimensionality.\n",
    "\n",
    "**Requirement 5.4**: Provide feature impact analysis showing performance changes from feature additions or removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_tr, y_tr, X_te, y_te):\n",
    "    \"\"\"Train a model and return classification metrics.\"\"\"\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_proba = model.predict_proba(X_te)[:, 1]\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_te, y_pred),\n",
    "        'precision': precision_score(y_te, y_pred),\n",
    "        'recall': recall_score(y_te, y_pred),\n",
    "        'f1': f1_score(y_te, y_pred),\n",
    "        'auc_roc': roc_auc_score(y_te, y_proba),\n",
    "    }\n",
    "\n",
    "\n",
    "# Model with ALL features\n",
    "model_all = XGBClassifier(\n",
    "    max_depth=5, learning_rate=0.2, n_estimators=100,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    use_label_encoder=False, eval_metric='logloss',\n",
    ")\n",
    "metrics_all = evaluate_model(model_all, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Model with SELECTED features (univariate top-K)\n",
    "model_selected = XGBClassifier(\n",
    "    max_depth=5, learning_rate=0.2, n_estimators=100,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    use_label_encoder=False, eval_metric='logloss',\n",
    ")\n",
    "metrics_selected = evaluate_model(\n",
    "    model_selected,\n",
    "    X_train[selected_univariate], y_train,\n",
    "    X_test[selected_univariate], y_test,\n",
    ")\n",
    "\n",
    "print(f'All features ({len(ALL_FEATURES)}):      {metrics_all}')\n",
    "print(f'Selected features ({K}): {metrics_selected}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics side by side\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': list(metrics_all.keys()),\n",
    "    f'All Features ({len(ALL_FEATURES)})': list(metrics_all.values()),\n",
    "    f'Selected Features ({K})': list(metrics_selected.values()),\n",
    "})\n",
    "comparison['Difference'] = comparison[f'Selected Features ({K})'] - comparison[f'All Features ({len(ALL_FEATURES)})']\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart: all features vs selected features\n",
    "metric_names = list(metrics_all.keys())\n",
    "x = np.arange(len(metric_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width / 2, list(metrics_all.values()), width,\n",
    "               label=f'All Features ({len(ALL_FEATURES)})', color='steelblue')\n",
    "bars2 = ax.bar(x + width / 2, list(metrics_selected.values()), width,\n",
    "               label=f'Selected Features ({K})', color='coral')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Feature Impact Analysis — All vs Selected Features')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.replace('_', ' ').title() for m in metric_names])\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_impact_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000028",
   "metadata": {},
   "source": [
    "## 10. Log Feature Engineering Experiment\n",
    "\n",
    "Log the feature engineering experiment to the `ExperimentTracker` so that the\n",
    "feature set, selection method, and resulting metrics are recorded for\n",
    "reproducibility.\n",
    "\n",
    "**Requirement 5.3**: Log the feature set used and resulting model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000029",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = ExperimentTracker(region_name='us-east-1')\n",
    "\n",
    "# Log experiment with all features\n",
    "exp_all = tracker.start_experiment(\n",
    "    experiment_name='feature-engineering',\n",
    "    algorithm='XGBoost',\n",
    "    description='Baseline with all engineered features',\n",
    ")\n",
    "tracker.log_parameters(exp_all, {\n",
    "    'n_features': len(ALL_FEATURES),\n",
    "    'selection_method': 'none',\n",
    "    'feature_set': 'all',\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.2,\n",
    "    'n_estimators': 100,\n",
    "})\n",
    "tracker.log_metrics(exp_all, metrics_all)\n",
    "tracker.close_experiment(exp_all)\n",
    "print(f'Logged all-features experiment: {exp_all}')\n",
    "\n",
    "# Log experiment with selected features\n",
    "exp_selected = tracker.start_experiment(\n",
    "    experiment_name='feature-engineering',\n",
    "    algorithm='XGBoost',\n",
    "    description=f'Univariate top-{K} selected features',\n",
    ")\n",
    "tracker.log_parameters(exp_selected, {\n",
    "    'n_features': K,\n",
    "    'selection_method': 'univariate_f_classif',\n",
    "    'feature_set': ', '.join(selected_univariate),\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.2,\n",
    "    'n_estimators': 100,\n",
    "})\n",
    "tracker.log_metrics(exp_selected, metrics_selected)\n",
    "tracker.close_experiment(exp_selected)\n",
    "print(f'Logged selected-features experiment: {exp_selected}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000030",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### What we covered\n",
    "\n",
    "| Step | Technique | New Features / Output |\n",
    "|------|-----------|----------------------|\n",
    "| Time features | `create_time_features` | `hour`, `day_of_week`, `is_weekend`, `is_night` |\n",
    "| Amount features | `create_amount_features` | `amount_log`, `amount_squared`, `amount_sqrt` |\n",
    "| Interaction features | `create_interaction_features` | `V1_x_V2`, `V3_x_V4`, `V1_x_Amount` |\n",
    "| Univariate selection | `select_features_univariate` | Top-K features by F-score |\n",
    "| RFE selection | `select_features_rfe` | Top-N features by recursive elimination |\n",
    "| Importance analysis | `analyze_feature_importance` | Random Forest Gini importance ranking |\n",
    "| Impact analysis | XGBoost comparison | All features vs selected features |\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "- Feature selection can maintain (or improve) model performance while reducing\n",
    "  dimensionality, leading to faster training and simpler models.\n",
    "- Different selection methods may agree on the most important features but\n",
    "  diverge on borderline ones — combining methods gives a more robust picture.\n",
    "- All experiments are logged to the ExperimentTracker for full reproducibility.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "1. **Tune hyperparameters** on the selected feature set using\n",
    "   `02_hyperparameter_tuning.ipynb`.\n",
    "2. **Compare algorithms** with the engineered features using\n",
    "   `03_algorithm_comparison.ipynb`.\n",
    "3. **Promote to production** via `05_production_promotion.ipynb` once the best\n",
    "   configuration is identified."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}