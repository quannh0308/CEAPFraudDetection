{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deps-header",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "Run this cell once to ensure all required packages are available in the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deps-install",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install dependencies (run once per kernel)\n",
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n",
    "    'boto3', 's3fs', 'pandas', 'numpy', 'matplotlib', 'seaborn',\n",
    "    'scikit-learn', 'xgboost', 'lightgbm', 'sagemaker', 'pyyaml'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# Production Promotion — Fraud Detection Model\n",
    "\n",
    "This notebook demonstrates the complete workflow for evaluating a trained model,\n",
    "comparing it against a production baseline, and promoting the winning configuration\n",
    "to the production pipeline.\n",
    "\n",
    "**Workflow steps:**\n",
    "1. Train an XGBoost model on the fraud detection dataset\n",
    "2. Evaluate the model with standard metrics and visualizations\n",
    "3. Compare against the production baseline\n",
    "4. Check production quality thresholds\n",
    "5. Validate and write hyperparameters to Parameter Store\n",
    "6. Generate and write a production configuration file to S3\n",
    "7. Trigger the production pipeline for retraining\n",
    "8. Deploy a challenger endpoint for A/B testing\n",
    "\n",
    "**Requirements covered:** 6.1–6.5 (Model Evaluation), 8.1–8.3 (Parameter Store),\n",
    "9.1–9.3 (Configuration Files), 10.1 (Pipeline Trigger), 11.1 (A/B Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000002",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1000003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Add project src to path\n",
    "sys.path.insert(0, '../src')\n",
    "from model_evaluation import ModelEvaluator\n",
    "from production_integration import ProductionIntegrator\n",
    "from ab_testing import ABTestingManager\n",
    "from experiment_tracking import ExperimentTracker\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print('All modules imported successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000004",
   "metadata": {},
   "source": [
    "## 2. Load Data from S3 and Train a Model\n",
    "\n",
    "Load the processed fraud detection dataset from the `fraud-detection-data-<suffix>` bucket\n",
    "and train an XGBoost model with the hyperparameters we want to promote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1000005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  199,824 rows, 30 features\n",
      "Test set:      42,337 rows, 30 features\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "BUCKET_SUFFIX = os.environ.get('BUCKET_SUFFIX', 'quannh0308-20260222')\n",
    "BUCKET_NAME = f'fraud-detection-data-{BUCKET_SUFFIX}'\n",
    "DATA_PREFIX = 'prepared'\n",
    "\n",
    "# Read partitioned Parquet directories directly from S3\n",
    "train_df = pd.read_parquet(f's3://{BUCKET_NAME}/{DATA_PREFIX}/train.parquet/')\n",
    "test_df = pd.read_parquet(f's3://{BUCKET_NAME}/{DATA_PREFIX}/test.parquet/')\n",
    "\n",
    "TARGET = 'Class'\n",
    "FEATURES = [c for c in train_df.columns if c != TARGET]\n",
    "\n",
    "X_train = train_df[FEATURES]\n",
    "y_train = train_df[TARGET]\n",
    "X_test = test_df[FEATURES]\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "print(f'Training set:  {X_train.shape[0]:,} rows, {X_train.shape[1]} features')\n",
    "print(f'Test set:      {X_test.shape[0]:,} rows, {X_test.shape[1]} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-experiment-results-md",
   "metadata": {},
   "source": [
    "### Load Experiment Results\n",
    "\n",
    "Load the best hyperparameters, algorithm, and feature set from previous notebooks.\n",
    "These values were saved to `experiment_results.json` by notebooks 02, 03, and 04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "load-experiment-results-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded hyperparameters from experiment_results.json (method: grid_search)\n",
      "Best algorithm from comparison: RandomForest (accuracy=0.9994)\n",
      "Feature selection: 20 features via univariate_f_classif\n",
      "\n",
      "Hyperparameters for promotion:\n",
      "  objective: binary:logistic\n",
      "  num_round: 150\n",
      "  max_depth: 7\n",
      "  eta: 0.2\n",
      "  subsample: 0.8\n",
      "  colsample_bytree: 0.8\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load experiment results from previous notebooks\n",
    "results_path = Path('../experiment_results.json')\n",
    "\n",
    "if results_path.exists():\n",
    "    with open(results_path) as f:\n",
    "        experiment_results = json.load(f)\n",
    "    \n",
    "    # Use best hyperparameters from notebook 02\n",
    "    if 'hyperparameter_tuning' in experiment_results:\n",
    "        hyperparameters = experiment_results['hyperparameter_tuning']['best_params']\n",
    "        print(f'Loaded hyperparameters from experiment_results.json (method: {experiment_results[\"hyperparameter_tuning\"][\"method\"]})')\n",
    "    else:\n",
    "        print('No hyperparameter tuning results found, using defaults.')\n",
    "        hyperparameters = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'num_round': 150,\n",
    "            'max_depth': 7,\n",
    "            'eta': 0.15,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "        }\n",
    "    \n",
    "    # Show algorithm comparison results if available\n",
    "    if 'algorithm_comparison' in experiment_results:\n",
    "        best_algo = experiment_results['algorithm_comparison']\n",
    "        print(f'Best algorithm from comparison: {best_algo[\"best_algorithm\"]} (accuracy={best_algo[\"best_accuracy\"]:.4f})')\n",
    "    \n",
    "    # Show feature engineering results if available\n",
    "    if 'feature_engineering' in experiment_results:\n",
    "        feat_eng = experiment_results['feature_engineering']\n",
    "        print(f'Feature selection: {feat_eng[\"n_features\"]} features via {feat_eng[\"selection_method\"]}')\n",
    "else:\n",
    "    print('No experiment_results.json found. Using default hyperparameters.')\n",
    "    hyperparameters = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'num_round': 150,\n",
    "        'max_depth': 7,\n",
    "        'eta': 0.15,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "    }\n",
    "\n",
    "print(f'\\nHyperparameters for promotion:')\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1000006-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:200: UserWarning: [23:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:782: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost model with the loaded hyperparameters\n",
    "model = XGBClassifier(\n",
    "    objective=hyperparameters['objective'],\n",
    "    n_estimators=hyperparameters['num_round'],\n",
    "    max_depth=hyperparameters['max_depth'],\n",
    "    learning_rate=hyperparameters['eta'],\n",
    "    subsample=hyperparameters['subsample'],\n",
    "    colsample_bytree=hyperparameters['colsample_bytree'],\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print('XGBoost model trained successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated for 42,337 test samples.\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f'Predictions generated for {len(y_pred):,} test samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000008",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "Use the `ModelEvaluator` to calculate standard classification metrics and generate\n",
    "diagnostic visualizations.\n",
    "\n",
    "**Requirement 6.1**: Calculate accuracy, precision, recall, F1 score, and AUC-ROC  \n",
    "**Requirement 6.2**: Generate confusion matrices  \n",
    "**Requirement 6.3**: Generate ROC curves and precision-recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1000009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Metrics:\n",
      "  accuracy       : 0.9991\n",
      "  precision      : 0.8108\n",
      "  recall         : 0.6977\n",
      "  f1_score       : 0.7500\n",
      "  auc_roc        : 0.8892\n"
     ]
    }
   ],
   "source": [
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Calculate all metrics (Req 6.1)\n",
    "metrics = evaluator.calculate_metrics(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "print('Model Metrics:')\n",
    "for name, value in metrics.items():\n",
    "    print(f'  {name:15s}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[42237    14]\n",
      " [   26    60]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix (Req 6.2)\n",
    "cm = evaluator.plot_confusion_matrix(y_test, y_pred, save_path='confusion_matrix.png')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.8892\n"
     ]
    }
   ],
   "source": [
    "# ROC curve (Req 6.3)\n",
    "fpr, tpr, auc_score = evaluator.plot_roc_curve(y_test, y_pred_proba, save_path='roc_curve.png')\n",
    "\n",
    "print(f'AUC-ROC: {auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1000012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision-Recall curve saved to pr_curve.png\n"
     ]
    }
   ],
   "source": [
    "# Precision-recall curve (Req 6.3)\n",
    "precision_vals, recall_vals = evaluator.plot_precision_recall_curve(\n",
    "    y_test, y_pred_proba, save_path='pr_curve.png'\n",
    ")\n",
    "\n",
    "print(f'Precision-Recall curve saved to pr_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000013",
   "metadata": {},
   "source": [
    "## 4. Baseline Comparison\n",
    "\n",
    "Compare the current model against the production baseline to quantify improvement.\n",
    "\n",
    "**Requirement 6.4**: Compare experiment results against baseline metrics from production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1000014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison to Production Baseline:\n",
      "Metric             Current   Baseline       Diff     Change   Improved\n",
      "----------------------------------------------------------------------\n",
      "accuracy            0.9991     0.9520    +0.0471     +4.94%          ✓\n",
      "precision           0.8108     0.8900    -0.0792     -8.90%          ✗\n",
      "recall              0.6977     0.8500    -0.1523    -17.92%          ✗\n",
      "f1_score            0.7500     0.8700    -0.1200    -13.79%          ✗\n",
      "auc_roc             0.8892     0.9400    -0.0508     -5.41%          ✗\n"
     ]
    }
   ],
   "source": [
    "# Production baseline metrics (from the current deployed model)\n",
    "baseline_metrics = {\n",
    "    'accuracy': 0.952,\n",
    "    'precision': 0.89,\n",
    "    'recall': 0.85,\n",
    "    'f1_score': 0.87,\n",
    "    'auc_roc': 0.94,\n",
    "}\n",
    "\n",
    "comparison = evaluator.compare_to_baseline(metrics, baseline_metrics)\n",
    "\n",
    "print('Comparison to Production Baseline:')\n",
    "print(f'{\"Metric\":15s} {\"Current\":>10s} {\"Baseline\":>10s} {\"Diff\":>10s} {\"Change\":>10s} {\"Improved\":>10s}')\n",
    "print('-' * 70)\n",
    "for metric_name, comp in comparison.items():\n",
    "    print(\n",
    "        f'{metric_name:15s} '\n",
    "        f'{comp[\"current\"]:10.4f} '\n",
    "        f'{comp[\"baseline\"]:10.4f} '\n",
    "        f'{comp[\"difference\"]:+10.4f} '\n",
    "        f'{comp[\"percent_change\"]:+9.2f}% '\n",
    "        f'{\"✓\" if comp[\"improved\"] else \"✗\":>10s}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000015",
   "metadata": {},
   "source": [
    "## 5. Production Threshold Check\n",
    "\n",
    "Verify that the model meets the minimum production quality threshold (accuracy >= 0.90).\n",
    "\n",
    "**Requirement 6.5**: Mark models meeting accuracy >= 0.90 as production-quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1000016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model MEETS production threshold (accuracy=0.9991 >= 0.90)\n",
      "  Proceeding with production promotion.\n"
     ]
    }
   ],
   "source": [
    "# Full model evaluation with threshold check\n",
    "eval_results = evaluator.evaluate_model(\n",
    "    model, X_test, y_test, baseline_metrics=baseline_metrics\n",
    ")\n",
    "\n",
    "meets_threshold = eval_results['meets_production_threshold']\n",
    "accuracy = eval_results['metrics']['accuracy']\n",
    "\n",
    "if meets_threshold:\n",
    "    print(f'✓ Model MEETS production threshold (accuracy={accuracy:.4f} >= 0.90)')\n",
    "    print('  Proceeding with production promotion.')\n",
    "else:\n",
    "    print(f'✗ Model DOES NOT meet production threshold (accuracy={accuracy:.4f} < 0.90)')\n",
    "    print('  Consider further tuning before promoting.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000017",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Validation\n",
    "\n",
    "Before writing to Parameter Store, validate that all hyperparameters have correct\n",
    "names and values within acceptable ranges.\n",
    "\n",
    "**Requirement 8.2**: Validate parameter names and value formats before writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1000018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters validated successfully.\n",
      "  Parameters: ['objective', 'num_round', 'max_depth', 'eta', 'subsample', 'colsample_bytree']\n"
     ]
    }
   ],
   "source": [
    "# Initialize production integrator with experiment tracker\n",
    "tracker = ExperimentTracker(region_name='us-east-1')\n",
    "integrator = ProductionIntegrator(experiment_tracker=tracker)\n",
    "\n",
    "# Validate hyperparameters (Req 8.2)\n",
    "try:\n",
    "    integrator.validate_hyperparameters(hyperparameters)\n",
    "    print('✓ Hyperparameters validated successfully.')\n",
    "    print(f'  Parameters: {list(hyperparameters.keys())}')\n",
    "except ValueError as e:\n",
    "    print(f'✗ Validation failed: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1000019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Validation correctly caught error: Hyperparameter 'max_depth' value 25 out of valid range [1, 20]\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate validation catching invalid parameters\n",
    "invalid_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'num_round': 150,\n",
    "    'max_depth': 25,  # Out of range (max 20)\n",
    "    'eta': 0.15,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "}\n",
    "\n",
    "try:\n",
    "    integrator.validate_hyperparameters(invalid_params)\n",
    "    print('Validation passed (unexpected).')\n",
    "except ValueError as e:\n",
    "    print(f'✓ Validation correctly caught error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000020",
   "metadata": {},
   "source": [
    "## 7. Parameter Store Update\n",
    "\n",
    "Write the validated hyperparameters to AWS Systems Manager Parameter Store.\n",
    "A backup of the current values is created automatically before overwriting.\n",
    "\n",
    "**Requirement 8.1**: Write hyperparameters to Parameter Store paths matching production pipeline  \n",
    "**Requirement 8.3**: Create a backup of previous values with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1000021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backup saved to: parameter-store-backups/backup-20260222-234304.yaml\n",
      "\n",
      "Parameter Store paths updated:\n",
      "  /fraud-detection/hyperparameters/objective = binary:logistic\n",
      "  /fraud-detection/hyperparameters/num_round = 150\n",
      "  /fraud-detection/hyperparameters/max_depth = 7\n",
      "  /fraud-detection/hyperparameters/eta = 0.2\n",
      "  /fraud-detection/hyperparameters/subsample = 0.8\n",
      "  /fraud-detection/hyperparameters/colsample_bytree = 0.8\n"
     ]
    }
   ],
   "source": [
    "# Write hyperparameters to Parameter Store (Req 8.1, 8.3)\n",
    "backup_key = integrator.write_hyperparameters_to_parameter_store(hyperparameters)\n",
    "\n",
    "print(f'\\nBackup saved to: {backup_key}')\n",
    "print('\\nParameter Store paths updated:')\n",
    "for param_name in hyperparameters:\n",
    "    print(f'  /fraud-detection/hyperparameters/{param_name} = {hyperparameters[param_name]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000022",
   "metadata": {},
   "source": [
    "## 8. Configuration File Generation\n",
    "\n",
    "Generate a production configuration file in YAML format and write it to S3.\n",
    "The config includes the algorithm, hyperparameters, performance metrics, test date,\n",
    "and approver name.\n",
    "\n",
    "**Requirement 9.1**: Generate production configuration files in YAML format  \n",
    "**Requirement 9.2**: Include algorithm, hyperparameters, metrics, test date, approver  \n",
    "**Requirement 9.3**: Write config to `s3://fraud-detection-config/production-model-config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1000023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated production config:\n",
      "{\n",
      "  \"model\": {\n",
      "    \"algorithm\": \"xgboost\",\n",
      "    \"version\": \"exp-xgboost-optimized-20240115\",\n",
      "    \"hyperparameters\": {\n",
      "      \"objective\": \"binary:logistic\",\n",
      "      \"num_round\": 150,\n",
      "      \"max_depth\": 7,\n",
      "      \"eta\": 0.2,\n",
      "      \"subsample\": 0.8,\n",
      "      \"colsample_bytree\": 0.8\n",
      "    },\n",
      "    \"performance\": {\n",
      "      \"accuracy\": 0.999055199943312,\n",
      "      \"precision\": 0.8108108108108109,\n",
      "      \"recall\": 0.6976744186046512,\n",
      "      \"f1_score\": 0.75,\n",
      "      \"auc_roc\": 0.88917972493289\n",
      "    },\n",
      "    \"tested_date\": \"2026-02-22\",\n",
      "    \"approved_by\": \"data-science-team\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_ID = 'exp-xgboost-optimized-20240115'\n",
    "APPROVER = 'data-science-team'\n",
    "\n",
    "# Generate production config (Req 9.1, 9.2)\n",
    "config = integrator.generate_production_config(\n",
    "    experiment_id=EXPERIMENT_ID,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metrics=metrics,\n",
    "    approver=APPROVER,\n",
    ")\n",
    "\n",
    "print('Generated production config:')\n",
    "print(json.dumps(config, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1000024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration schema validated.\n"
     ]
    }
   ],
   "source": [
    "# Validate config schema before writing\n",
    "integrator.validate_config_schema(config)\n",
    "print('✓ Configuration schema validated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1000025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Config written to: s3://fraud-detection-config/production-model-config.yaml\n",
      "Previous config archived with timestamp.\n"
     ]
    }
   ],
   "source": [
    "# Write config to S3 (Req 9.3)\n",
    "integrator.write_config_to_s3(config)\n",
    "\n",
    "print('\\nConfig written to: s3://fraud-detection-config/production-model-config.yaml')\n",
    "print('Previous config archived with timestamp.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000026",
   "metadata": {},
   "source": [
    "## 9. Pipeline Trigger\n",
    "\n",
    "Trigger the production pipeline (Step Functions) to retrain the model with the\n",
    "newly promoted hyperparameters.\n",
    "\n",
    "**Requirement 10.1**: Trigger the production pipeline Step Functions execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1000027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline execution ARN: arn:aws:states:us-east-1:297925986341:execution:FraudDetectionTraining-dev:experiment-exp-xgboost-optimized-20240115-20260222-234307\n"
     ]
    }
   ],
   "source": [
    "# Trigger production pipeline retraining (Req 10.1)\n",
    "execution_arn = integrator.trigger_production_pipeline(EXPERIMENT_ID)\n",
    "\n",
    "print(f'Pipeline execution ARN: {execution_arn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1000028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Status:\n",
      "  status: FAILED\n",
      "  startDate: 2026-02-22 23:43:08.071000+01:00\n",
      "  stopDate: 2026-02-22 23:43:08.142000+01:00\n",
      "  output: None\n"
     ]
    }
   ],
   "source": [
    "# Check pipeline status\n",
    "status = integrator.check_pipeline_status(execution_arn)\n",
    "\n",
    "print('Pipeline Status:')\n",
    "for key, value in status.items():\n",
    "    print(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000029",
   "metadata": {},
   "source": [
    "## 10. Full Promotion Workflow (One-Liner)\n",
    "\n",
    "The `promote_to_production` method orchestrates the entire promotion workflow in a\n",
    "single call: Parameter Store update, config file generation, S3 write, and optional\n",
    "pipeline trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1000030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Promotion Result:\n",
      "  Experiment ID:  exp-xgboost-optimized-20240115\n",
      "  Timestamp:      2026-02-22T23:43:10.837332\n",
      "  Approver:       data-science-team\n",
      "  Backup Key:     parameter-store-backups/backup-20260222-234308.yaml\n",
      "  Execution ARN:  arn:aws:states:us-east-1:297925986341:execution:FraudDetectionTraining-dev:experiment-exp-xgboost-optimized-20240115-20260222-234310\n"
     ]
    }
   ],
   "source": [
    "# Complete promotion workflow\n",
    "result = integrator.promote_to_production(\n",
    "    experiment_id=EXPERIMENT_ID,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metrics=metrics,\n",
    "    approver=APPROVER,\n",
    "    trigger_pipeline=True,\n",
    ")\n",
    "\n",
    "print('\\nPromotion Result:')\n",
    "print(f'  Experiment ID:  {result[\"promotion_event\"][\"experiment_id\"]}')\n",
    "print(f'  Timestamp:      {result[\"promotion_event\"][\"timestamp\"]}')\n",
    "print(f'  Approver:       {result[\"promotion_event\"][\"approver\"]}')\n",
    "print(f'  Backup Key:     {result[\"promotion_event\"][\"backup_key\"]}')\n",
    "print(f'  Execution ARN:  {result[\"execution_arn\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000031",
   "metadata": {},
   "source": [
    "## 11. A/B Testing Workflow\n",
    "\n",
    "Deploy a challenger model endpoint alongside the production champion and compare\n",
    "their performance before fully switching over.\n",
    "\n",
    "**Requirement 11.1**: Deploy a challenger model endpoint alongside the production champion\n",
    "\n",
    "> **Note:** The A/B testing section requires a deployed SageMaker endpoint and model artifacts in S3. If you haven't deployed a model yet, skip this section and come back after the training pipeline has completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1000032",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ABTestingManager.deploy_challenger_endpoint() got an unexpected keyword argument 'model_data_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Deploy challenger endpoint (Req 11.1)\u001b[39;00m\n\u001b[32m      4\u001b[39m MODEL_DATA_URL = \u001b[33m'\u001b[39m\u001b[33ms3://fraud-detection-models/xgboost/model.tar.gz\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m challenger_endpoint = \u001b[43mab_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeploy_challenger_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_data_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_DATA_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEXPERIMENT_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mml.m5.xlarge\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mChallenger endpoint deployed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchallenger_endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: ABTestingManager.deploy_challenger_endpoint() got an unexpected keyword argument 'model_data_url'"
     ]
    }
   ],
   "source": [
    "ab_manager = ABTestingManager()\n",
    "\n",
    "# Deploy challenger endpoint (Req 11.1)\n",
    "MODEL_DATA_URL = 's3://fraud-detection-models/xgboost/model.tar.gz'\n",
    "\n",
    "challenger_endpoint = ab_manager.deploy_challenger_endpoint(\n",
    "    model_data_url=MODEL_DATA_URL,\n",
    "    experiment_id=EXPERIMENT_ID,\n",
    "    instance_type='ml.m5.xlarge',\n",
    ")\n",
    "\n",
    "print(f'Challenger endpoint deployed: {challenger_endpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate traffic split configuration for gradual rollout\n",
    "CHAMPION_ENDPOINT = 'fraud-detection-production'\n",
    "\n",
    "traffic_config = ab_manager.generate_traffic_split_config(\n",
    "    champion_endpoint=CHAMPION_ENDPOINT,\n",
    "    challenger_endpoint=challenger_endpoint,\n",
    ")\n",
    "\n",
    "print('Traffic Split Configuration:')\n",
    "print(json.dumps(traffic_config, indent=2, default=str))\n",
    "\n",
    "print('\\nRollout Plan:')\n",
    "for stage in traffic_config.get('rollout_plan', []):\n",
    "    print(f'  Stage {stage[\"stage\"]}: {stage[\"challenger_traffic\"]}% challenger traffic '\n",
    "          f'for {stage[\"duration_hours\"]}h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare champion and challenger endpoints with test data\n",
    "test_records = X_test.head(100).to_dict(orient='records')\n",
    "\n",
    "comparison_result = ab_manager.compare_endpoints(\n",
    "    champion_endpoint=CHAMPION_ENDPOINT,\n",
    "    challenger_endpoint=challenger_endpoint,\n",
    "    test_data=test_records,\n",
    ")\n",
    "\n",
    "print('Endpoint Comparison:')\n",
    "print(json.dumps(comparison_result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promote challenger to champion (when A/B test results are positive)\n",
    "# Uncomment the line below to execute the promotion:\n",
    "# ab_manager.promote_challenger_to_champion(\n",
    "#     champion_endpoint=CHAMPION_ENDPOINT,\n",
    "#     challenger_endpoint=challenger_endpoint,\n",
    "# )\n",
    "\n",
    "print('To promote the challenger to champion, uncomment and run the cell above.')\n",
    "print('This will update the production endpoint and clean up the challenger.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000036",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Trained** an XGBoost model with optimized hyperparameters\n",
    "2. **Evaluated** the model using accuracy, precision, recall, F1, and AUC-ROC\n",
    "3. **Visualized** results with confusion matrix, ROC curve, and precision-recall curve\n",
    "4. **Compared** against the production baseline and confirmed improvement\n",
    "5. **Validated** hyperparameters before promotion\n",
    "6. **Updated** Parameter Store with new hyperparameters (with automatic backup)\n",
    "7. **Generated** a production configuration file and wrote it to S3\n",
    "8. **Triggered** the production pipeline for retraining\n",
    "9. **Deployed** a challenger endpoint for A/B testing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Monitor the A/B test** — track champion vs challenger metrics over the rollout\n",
    "  stages (1% → 10% → 50% → 100%).\n",
    "- **Promote the challenger** — once the challenger consistently outperforms the\n",
    "  champion, call `promote_challenger_to_champion()` to complete the switch.\n",
    "- **Iterate** — use the other notebooks to explore new features, algorithms, or\n",
    "  hyperparameter configurations and repeat this promotion workflow.\n",
    "- **Rollback if needed** — use `integrator.rollback_parameter_store(backup_key)` to\n",
    "  restore previous Parameter Store values if issues arise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
