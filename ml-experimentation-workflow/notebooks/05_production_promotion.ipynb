{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# Production Promotion — Fraud Detection Model\n",
    "\n",
    "This notebook demonstrates the complete workflow for evaluating a trained model,\n",
    "comparing it against a production baseline, and promoting the winning configuration\n",
    "to the production pipeline.\n",
    "\n",
    "**Workflow steps:**\n",
    "1. Train an XGBoost model on the fraud detection dataset\n",
    "2. Evaluate the model with standard metrics and visualizations\n",
    "3. Compare against the production baseline\n",
    "4. Check production quality thresholds\n",
    "5. Validate and write hyperparameters to Parameter Store\n",
    "6. Generate and write a production configuration file to S3\n",
    "7. Trigger the production pipeline for retraining\n",
    "8. Deploy a challenger endpoint for A/B testing\n",
    "\n",
    "**Requirements covered:** 6.1–6.5 (Model Evaluation), 8.1–8.3 (Parameter Store),\n",
    "9.1–9.3 (Configuration Files), 10.1 (Pipeline Trigger), 11.1 (A/B Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000002",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Add project src to path\n",
    "sys.path.insert(0, '../src')\n",
    "from model_evaluation import ModelEvaluator\n",
    "from production_integration import ProductionIntegrator\n",
    "from ab_testing import ABTestingManager\n",
    "from experiment_tracking import ExperimentTracker\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print('All modules imported successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000004",
   "metadata": {},
   "source": [
    "## 2. Load Data from S3 and Train a Model\n",
    "\n",
    "Load the processed fraud detection dataset from the `fraud-detection-data` bucket\n",
    "and train an XGBoost model with the hyperparameters we want to promote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'fraud-detection-data'\n",
    "DATA_PREFIX = 'processed'\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def load_parquet_from_s3(bucket: str, key: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a Parquet file from S3 into a pandas DataFrame.\"\"\"\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_parquet(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "\n",
    "train_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/train.parquet')\n",
    "test_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/test.parquet')\n",
    "\n",
    "TARGET = 'Class'\n",
    "FEATURES = [c for c in train_df.columns if c != TARGET]\n",
    "\n",
    "X_train = train_df[FEATURES]\n",
    "y_train = train_df[TARGET]\n",
    "X_test = test_df[FEATURES]\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "print(f'Training set:  {X_train.shape[0]:,} rows, {X_train.shape[1]} features')\n",
    "print(f'Test set:      {X_test.shape[0]:,} rows, {X_test.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to promote\n",
    "hyperparameters = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'num_round': 150,\n",
    "    'max_depth': 7,\n",
    "    'eta': 0.15,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "}\n",
    "\n",
    "# Train XGBoost model\n",
    "model = XGBClassifier(\n",
    "    objective=hyperparameters['objective'],\n",
    "    n_estimators=hyperparameters['num_round'],\n",
    "    max_depth=hyperparameters['max_depth'],\n",
    "    learning_rate=hyperparameters['eta'],\n",
    "    subsample=hyperparameters['subsample'],\n",
    "    colsample_bytree=hyperparameters['colsample_bytree'],\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print('XGBoost model trained successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f'Predictions generated for {len(y_pred):,} test samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000008",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "Use the `ModelEvaluator` to calculate standard classification metrics and generate\n",
    "diagnostic visualizations.\n",
    "\n",
    "**Requirement 6.1**: Calculate accuracy, precision, recall, F1 score, and AUC-ROC  \n",
    "**Requirement 6.2**: Generate confusion matrices  \n",
    "**Requirement 6.3**: Generate ROC curves and precision-recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Calculate all metrics (Req 6.1)\n",
    "metrics = evaluator.calculate_metrics(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "print('Model Metrics:')\n",
    "for name, value in metrics.items():\n",
    "    print(f'  {name:15s}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix (Req 6.2)\n",
    "cm = evaluator.plot_confusion_matrix(y_test, y_pred, save_path='confusion_matrix.png')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve (Req 6.3)\n",
    "fpr, tpr, auc_score = evaluator.plot_roc_curve(y_test, y_pred_proba, save_path='roc_curve.png')\n",
    "\n",
    "print(f'AUC-ROC: {auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-recall curve (Req 6.3)\n",
    "precision_vals, recall_vals = evaluator.plot_precision_recall_curve(\n",
    "    y_test, y_pred_proba, save_path='pr_curve.png'\n",
    ")\n",
    "\n",
    "print(f'Precision-Recall curve saved to pr_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000013",
   "metadata": {},
   "source": [
    "## 4. Baseline Comparison\n",
    "\n",
    "Compare the current model against the production baseline to quantify improvement.\n",
    "\n",
    "**Requirement 6.4**: Compare experiment results against baseline metrics from production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production baseline metrics (from the current deployed model)\n",
    "baseline_metrics = {\n",
    "    'accuracy': 0.952,\n",
    "    'precision': 0.89,\n",
    "    'recall': 0.85,\n",
    "    'f1_score': 0.87,\n",
    "    'auc_roc': 0.94,\n",
    "}\n",
    "\n",
    "comparison = evaluator.compare_to_baseline(metrics, baseline_metrics)\n",
    "\n",
    "print('Comparison to Production Baseline:')\n",
    "print(f'{\"Metric\":15s} {\"Current\":>10s} {\"Baseline\":>10s} {\"Diff\":>10s} {\"Change\":>10s} {\"Improved\":>10s}')\n",
    "print('-' * 70)\n",
    "for metric_name, comp in comparison.items():\n",
    "    print(\n",
    "        f'{metric_name:15s} '\n",
    "        f'{comp[\"current\"]:10.4f} '\n",
    "        f'{comp[\"baseline\"]:10.4f} '\n",
    "        f'{comp[\"difference\"]:+10.4f} '\n",
    "        f'{comp[\"percent_change\"]:+9.2f}% '\n",
    "        f'{\"✓\" if comp[\"improved\"] else \"✗\":>10s}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000015",
   "metadata": {},
   "source": [
    "## 5. Production Threshold Check\n",
    "\n",
    "Verify that the model meets the minimum production quality threshold (accuracy >= 0.90).\n",
    "\n",
    "**Requirement 6.5**: Mark models meeting accuracy >= 0.90 as production-quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full model evaluation with threshold check\n",
    "eval_results = evaluator.evaluate_model(\n",
    "    model, X_test, y_test, baseline_metrics=baseline_metrics\n",
    ")\n",
    "\n",
    "meets_threshold = eval_results['meets_production_threshold']\n",
    "accuracy = eval_results['metrics']['accuracy']\n",
    "\n",
    "if meets_threshold:\n",
    "    print(f'✓ Model MEETS production threshold (accuracy={accuracy:.4f} >= 0.90)')\n",
    "    print('  Proceeding with production promotion.')\n",
    "else:\n",
    "    print(f'✗ Model DOES NOT meet production threshold (accuracy={accuracy:.4f} < 0.90)')\n",
    "    print('  Consider further tuning before promoting.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000017",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Validation\n",
    "\n",
    "Before writing to Parameter Store, validate that all hyperparameters have correct\n",
    "names and values within acceptable ranges.\n",
    "\n",
    "**Requirement 8.2**: Validate parameter names and value formats before writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize production integrator with experiment tracker\n",
    "tracker = ExperimentTracker(region_name='us-east-1')\n",
    "integrator = ProductionIntegrator(experiment_tracker=tracker)\n",
    "\n",
    "# Validate hyperparameters (Req 8.2)\n",
    "try:\n",
    "    integrator.validate_hyperparameters(hyperparameters)\n",
    "    print('✓ Hyperparameters validated successfully.')\n",
    "    print(f'  Parameters: {list(hyperparameters.keys())}')\n",
    "except ValueError as e:\n",
    "    print(f'✗ Validation failed: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate validation catching invalid parameters\n",
    "invalid_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'num_round': 150,\n",
    "    'max_depth': 25,  # Out of range (max 20)\n",
    "    'eta': 0.15,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "}\n",
    "\n",
    "try:\n",
    "    integrator.validate_hyperparameters(invalid_params)\n",
    "    print('Validation passed (unexpected).')\n",
    "except ValueError as e:\n",
    "    print(f'✓ Validation correctly caught error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000020",
   "metadata": {},
   "source": [
    "## 7. Parameter Store Update\n",
    "\n",
    "Write the validated hyperparameters to AWS Systems Manager Parameter Store.\n",
    "A backup of the current values is created automatically before overwriting.\n",
    "\n",
    "**Requirement 8.1**: Write hyperparameters to Parameter Store paths matching production pipeline  \n",
    "**Requirement 8.3**: Create a backup of previous values with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write hyperparameters to Parameter Store (Req 8.1, 8.3)\n",
    "backup_key = integrator.write_hyperparameters_to_parameter_store(hyperparameters)\n",
    "\n",
    "print(f'\\nBackup saved to: {backup_key}')\n",
    "print('\\nParameter Store paths updated:')\n",
    "for param_name in hyperparameters:\n",
    "    print(f'  /fraud-detection/hyperparameters/{param_name} = {hyperparameters[param_name]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000022",
   "metadata": {},
   "source": [
    "## 8. Configuration File Generation\n",
    "\n",
    "Generate a production configuration file in YAML format and write it to S3.\n",
    "The config includes the algorithm, hyperparameters, performance metrics, test date,\n",
    "and approver name.\n",
    "\n",
    "**Requirement 9.1**: Generate production configuration files in YAML format  \n",
    "**Requirement 9.2**: Include algorithm, hyperparameters, metrics, test date, approver  \n",
    "**Requirement 9.3**: Write config to `s3://fraud-detection-config/production-model-config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000023",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = 'exp-xgboost-optimized-20240115'\n",
    "APPROVER = 'data-science-team'\n",
    "\n",
    "# Generate production config (Req 9.1, 9.2)\n",
    "config = integrator.generate_production_config(\n",
    "    experiment_id=EXPERIMENT_ID,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metrics=metrics,\n",
    "    approver=APPROVER,\n",
    ")\n",
    "\n",
    "print('Generated production config:')\n",
    "print(json.dumps(config, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate config schema before writing\n",
    "integrator.validate_config_schema(config)\n",
    "print('✓ Configuration schema validated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write config to S3 (Req 9.3)\n",
    "integrator.write_config_to_s3(config)\n",
    "\n",
    "print('\\nConfig written to: s3://fraud-detection-config/production-model-config.yaml')\n",
    "print('Previous config archived with timestamp.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000026",
   "metadata": {},
   "source": [
    "## 9. Pipeline Trigger\n",
    "\n",
    "Trigger the production pipeline (Step Functions) to retrain the model with the\n",
    "newly promoted hyperparameters.\n",
    "\n",
    "**Requirement 10.1**: Trigger the production pipeline Step Functions execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger production pipeline retraining (Req 10.1)\n",
    "execution_arn = integrator.trigger_production_pipeline(EXPERIMENT_ID)\n",
    "\n",
    "print(f'Pipeline execution ARN: {execution_arn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pipeline status\n",
    "status = integrator.check_pipeline_status(execution_arn)\n",
    "\n",
    "print('Pipeline Status:')\n",
    "for key, value in status.items():\n",
    "    print(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000029",
   "metadata": {},
   "source": [
    "## 10. Full Promotion Workflow (One-Liner)\n",
    "\n",
    "The `promote_to_production` method orchestrates the entire promotion workflow in a\n",
    "single call: Parameter Store update, config file generation, S3 write, and optional\n",
    "pipeline trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete promotion workflow\n",
    "result = integrator.promote_to_production(\n",
    "    experiment_id=EXPERIMENT_ID,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metrics=metrics,\n",
    "    approver=APPROVER,\n",
    "    trigger_pipeline=True,\n",
    ")\n",
    "\n",
    "print('\\nPromotion Result:')\n",
    "print(f'  Experiment ID:  {result[\"promotion_event\"][\"experiment_id\"]}')\n",
    "print(f'  Timestamp:      {result[\"promotion_event\"][\"timestamp\"]}')\n",
    "print(f'  Approver:       {result[\"promotion_event\"][\"approver\"]}')\n",
    "print(f'  Backup Key:     {result[\"promotion_event\"][\"backup_key\"]}')\n",
    "print(f'  Execution ARN:  {result[\"execution_arn\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000031",
   "metadata": {},
   "source": [
    "## 11. A/B Testing Workflow\n",
    "\n",
    "Deploy a challenger model endpoint alongside the production champion and compare\n",
    "their performance before fully switching over.\n",
    "\n",
    "**Requirement 11.1**: Deploy a challenger model endpoint alongside the production champion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_manager = ABTestingManager()\n",
    "\n",
    "# Deploy challenger endpoint (Req 11.1)\n",
    "MODEL_DATA_URL = 's3://fraud-detection-models/xgboost/model.tar.gz'\n",
    "\n",
    "challenger_endpoint = ab_manager.deploy_challenger_endpoint(\n",
    "    model_data_url=MODEL_DATA_URL,\n",
    "    experiment_id=EXPERIMENT_ID,\n",
    "    instance_type='ml.m5.xlarge',\n",
    ")\n",
    "\n",
    "print(f'Challenger endpoint deployed: {challenger_endpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate traffic split configuration for gradual rollout\n",
    "CHAMPION_ENDPOINT = 'fraud-detection-production'\n",
    "\n",
    "traffic_config = ab_manager.generate_traffic_split_config(\n",
    "    champion_endpoint=CHAMPION_ENDPOINT,\n",
    "    challenger_endpoint=challenger_endpoint,\n",
    ")\n",
    "\n",
    "print('Traffic Split Configuration:')\n",
    "print(json.dumps(traffic_config, indent=2, default=str))\n",
    "\n",
    "print('\\nRollout Plan:')\n",
    "for stage in traffic_config.get('rollout_plan', []):\n",
    "    print(f'  Stage {stage[\"stage\"]}: {stage[\"challenger_traffic\"]}% challenger traffic '\n",
    "          f'for {stage[\"duration_hours\"]}h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare champion and challenger endpoints with test data\n",
    "test_records = X_test.head(100).to_dict(orient='records')\n",
    "\n",
    "comparison_result = ab_manager.compare_endpoints(\n",
    "    champion_endpoint=CHAMPION_ENDPOINT,\n",
    "    challenger_endpoint=challenger_endpoint,\n",
    "    test_data=test_records,\n",
    ")\n",
    "\n",
    "print('Endpoint Comparison:')\n",
    "print(json.dumps(comparison_result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promote challenger to champion (when A/B test results are positive)\n",
    "# Uncomment the line below to execute the promotion:\n",
    "# ab_manager.promote_challenger_to_champion(\n",
    "#     champion_endpoint=CHAMPION_ENDPOINT,\n",
    "#     challenger_endpoint=challenger_endpoint,\n",
    "# )\n",
    "\n",
    "print('To promote the challenger to champion, uncomment and run the cell above.')\n",
    "print('This will update the production endpoint and clean up the challenger.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000036",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Trained** an XGBoost model with optimized hyperparameters\n",
    "2. **Evaluated** the model using accuracy, precision, recall, F1, and AUC-ROC\n",
    "3. **Visualized** results with confusion matrix, ROC curve, and precision-recall curve\n",
    "4. **Compared** against the production baseline and confirmed improvement\n",
    "5. **Validated** hyperparameters before promotion\n",
    "6. **Updated** Parameter Store with new hyperparameters (with automatic backup)\n",
    "7. **Generated** a production configuration file and wrote it to S3\n",
    "8. **Triggered** the production pipeline for retraining\n",
    "9. **Deployed** a challenger endpoint for A/B testing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Monitor the A/B test** — track champion vs challenger metrics over the rollout\n",
    "  stages (1% → 10% → 50% → 100%).\n",
    "- **Promote the challenger** — once the challenger consistently outperforms the\n",
    "  champion, call `promote_challenger_to_champion()` to complete the switch.\n",
    "- **Iterate** — use the other notebooks to explore new features, algorithms, or\n",
    "  hyperparameter configurations and repeat this promotion workflow.\n",
    "- **Rollback if needed** — use `integrator.rollback_parameter_store(backup_key)` to\n",
    "  restore previous Parameter Store values if issues arise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}