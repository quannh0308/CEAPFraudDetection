{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Algorithm Comparison - Fraud Detection Model\n",
    "\n",
    "This notebook compares multiple ML algorithms on the fraud detection dataset using\n",
    "the project's `AlgorithmComparator` and `ExperimentTracker` classes.\n",
    "\n",
    "We train and evaluate four algorithms side by side:\n",
    "1. **XGBoost** — gradient boosted trees (current production algorithm)\n",
    "2. **LightGBM** — fast gradient boosting with leaf-wise growth\n",
    "3. **Random Forest** — bagged decision trees (interpretable baseline)\n",
    "4. **Neural Network** — multi-layer perceptron for tabular data\n",
    "\n",
    "All experiments are logged to the ExperimentTracker for reproducibility.\n",
    "\n",
    "**Requirements covered:** 4.1 (XGBoost), 4.2 (LightGBM), 4.3 (Random Forest), 4.4 (Neural Networks), 4.5 (Comparison visualizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project src to path\n",
    "sys.path.insert(0, '../src')\n",
    "from experiment_tracking import ExperimentTracker\n",
    "from algorithm_comparison import AlgorithmComparator\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## 2. Load Data from S3\n",
    "\n",
    "Load the processed fraud detection dataset from the `fraud-detection-data` bucket.\n",
    "We use the same train/test split that the production pipeline consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'fraud-detection-data'\n",
    "DATA_PREFIX = 'processed'\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def load_parquet_from_s3(bucket: str, key: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a Parquet file from S3 into a pandas DataFrame.\"\"\"\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_parquet(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "\n",
    "train_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/train.parquet')\n",
    "test_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/test.parquet')\n",
    "\n",
    "# Separate features and target\n",
    "TARGET = 'Class'\n",
    "FEATURES = [c for c in train_df.columns if c != TARGET]\n",
    "\n",
    "X_train = train_df[FEATURES]\n",
    "y_train = train_df[TARGET]\n",
    "X_test = test_df[FEATURES]\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "print(f'Training set:  {X_train.shape[0]:,} rows, {X_train.shape[1]} features')\n",
    "print(f'Test set:      {X_test.shape[0]:,} rows, {X_test.shape[1]} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "## 3. Initialize ExperimentTracker and AlgorithmComparator\n",
    "\n",
    "The `AlgorithmComparator` accepts an optional `ExperimentTracker`. When provided,\n",
    "every algorithm run is automatically logged with its parameters and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = ExperimentTracker(region_name='us-east-1')\n",
    "comparator = AlgorithmComparator(tracker=tracker)\n",
    "\n",
    "print('ExperimentTracker and AlgorithmComparator initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "## 4. Run Algorithm Comparison (Default Algorithms)\n",
    "\n",
    "Call `compare_algorithms` with `algorithms=None` to use the default set:\n",
    "XGBoost, LightGBM, Random Forest, and Neural Network (MLP).\n",
    "\n",
    "Each algorithm is trained on the same data, evaluated on the same test set, and\n",
    "logged to the ExperimentTracker.\n",
    "\n",
    "**Requirements 4.1–4.4**: Support training with XGBoost, LightGBM, Random Forest, and Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = comparator.compare_algorithms(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    algorithms=None,\n",
    "    experiment_name='algorithm-comparison',\n",
    ")\n",
    "\n",
    "print(f'Comparison complete — {len(results_df)} algorithms evaluated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "### 4.1 Comparison Results Table\n",
    "\n",
    "The returned DataFrame contains one row per algorithm with columns for each metric\n",
    "and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best algorithm by F1 score\n",
    "best_idx = results_df['f1'].idxmax()\n",
    "best_algo = results_df.loc[best_idx, 'algorithm']\n",
    "best_f1 = results_df.loc[best_idx, 'f1']\n",
    "\n",
    "print(f'Best algorithm by F1 score: {best_algo} ({best_f1:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## 5. Built-in Comparison Visualizations\n",
    "\n",
    "Use `visualize_comparison` to generate bar charts for all metrics.\n",
    "\n",
    "**Requirement 4.5**: Provide comparison visualizations (accuracy, precision, recall, F1, AUC, training time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparator.visualize_comparison(results_df, save_path='algorithm_comparison.png')\n",
    "\n",
    "# Display the saved figure inline\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename='algorithm_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "## 6. Custom Visualizations\n",
    "\n",
    "Beyond the built-in bar charts, we create a radar chart to compare algorithm\n",
    "profiles across all classification metrics at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart comparing classification metrics across algorithms\n",
    "radar_metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']\n",
    "available_radar = [m for m in radar_metrics if m in results_df.columns]\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(available_radar), endpoint=False).tolist()\n",
    "angles += angles[:1]  # close the polygon\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    values = [row[m] for m in available_radar]\n",
    "    values += values[:1]  # close the polygon\n",
    "    color = colors[idx % len(colors)]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['algorithm'], color=color)\n",
    "    ax.fill(angles, values, alpha=0.1, color=color)\n",
    "\n",
    "ax.set_thetagrids(\n",
    "    [a * 180 / np.pi for a in angles[:-1]],\n",
    "    [m.replace('_', ' ').title() for m in available_radar],\n",
    ")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Algorithm Comparison — Radar Chart', y=1.08, fontsize=14)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.savefig('algorithm_radar_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart: classification metrics side by side\n",
    "plot_metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']\n",
    "available_plot = [m for m in plot_metrics if m in results_df.columns]\n",
    "\n",
    "melted = results_df.melt(\n",
    "    id_vars='algorithm',\n",
    "    value_vars=available_plot,\n",
    "    var_name='metric',\n",
    "    value_name='score',\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=melted, x='metric', y='score', hue='algorithm')\n",
    "plt.title('Algorithm Comparison — Classification Metrics')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend(title='Algorithm')\n",
    "plt.tight_layout()\n",
    "plt.savefig('algorithm_grouped_bar.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time comparison\n",
    "if 'training_time_seconds' in results_df.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.barh(\n",
    "        results_df['algorithm'],\n",
    "        results_df['training_time_seconds'],\n",
    "        color=colors[:len(results_df)],\n",
    "    )\n",
    "    for bar, t in zip(bars, results_df['training_time_seconds']):\n",
    "        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height() / 2,\n",
    "                 f'{t:.2f}s', va='center')\n",
    "    plt.xlabel('Training Time (seconds)')\n",
    "    plt.title('Algorithm Comparison — Training Time')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('algorithm_training_time.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2",
   "metadata": {},
   "source": [
    "## 7. Custom Algorithm Configurations\n",
    "\n",
    "You can pass a custom dictionary of algorithms to `compare_algorithms` to test\n",
    "specific configurations or include additional models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "custom_algorithms = {\n",
    "    'XGBoost-Deep': XGBClassifier(\n",
    "        max_depth=10,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=200,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "    ),\n",
    "    'LightGBM-Fast': LGBMClassifier(\n",
    "        max_depth=3,\n",
    "        learning_rate=0.3,\n",
    "        n_estimators=50,\n",
    "        verbose=-1,\n",
    "    ),\n",
    "    'RandomForest-Large': RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=15,\n",
    "        max_features='sqrt',\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f'Custom comparison with {len(custom_algorithms)} algorithms.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_results_df = comparator.compare_algorithms(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    algorithms=custom_algorithms,\n",
    "    experiment_name='custom-algorithm-comparison',\n",
    ")\n",
    "\n",
    "custom_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize custom comparison\n",
    "comparator.visualize_comparison(custom_results_df, save_path='custom_algorithm_comparison.png')\n",
    "\n",
    "display(Image(filename='custom_algorithm_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations\n",
    "\n",
    "### Findings\n",
    "\n",
    "The default comparison evaluated four algorithms on the same fraud detection dataset.\n",
    "Key observations:\n",
    "\n",
    "- **XGBoost** and **LightGBM** typically achieve the highest AUC-ROC and F1 scores\n",
    "  for tabular fraud detection data.\n",
    "- **Random Forest** provides a solid interpretable baseline with competitive accuracy.\n",
    "- **Neural Network (MLP)** can match tree-based methods but is more sensitive to\n",
    "  hyperparameter choices and feature scaling.\n",
    "- Training time varies significantly — LightGBM is generally the fastest, while\n",
    "  Neural Networks take the longest.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Tune the winning algorithm** — use notebook `02_hyperparameter_tuning.ipynb`\n",
    "   to optimize the best-performing algorithm's hyperparameters.\n",
    "2. **Engineer features** — use notebook `04_feature_engineering.ipynb` to create\n",
    "   derived features that may improve performance.\n",
    "3. **Promote to production** — once satisfied, use notebook\n",
    "   `05_production_promotion.ipynb` to push the winning configuration to the\n",
    "   production pipeline via Parameter Store and configuration files.\n",
    "4. **A/B test** — deploy the challenger model alongside the production champion\n",
    "   to validate improvements on live traffic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}