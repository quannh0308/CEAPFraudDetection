{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning - Fraud Detection Model\n",
    "\n",
    "This notebook demonstrates hyperparameter tuning strategies for the fraud detection\n",
    "model using the project's `HyperparameterTuner` and `ExperimentTracker` classes.\n",
    "\n",
    "We cover three approaches:\n",
    "1. **Grid Search** — exhaustive search over a discrete parameter grid\n",
    "2. **Random Search** — sampling from parameter distributions (including callable lambdas)\n",
    "3. **SageMaker Automatic Model Tuning** — Bayesian optimization via managed tuning jobs\n",
    "\n",
    "All trials are automatically logged to the ExperimentTracker for reproducibility.\n",
    "\n",
    "**Requirements covered:** 3.1 (Grid search), 3.2 (Random search), 3.3 (SageMaker Automatic Model Tuning), 3.4 (Log all parameter combinations and metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import random\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Add project src to path\n",
    "sys.path.insert(0, '../src')\n",
    "from experiment_tracking import ExperimentTracker\n",
    "from hyperparameter_tuning import HyperparameterTuner\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## 2. Load Data from S3\n",
    "\n",
    "Load the fraud detection dataset from the `fraud-detection-data` bucket and prepare\n",
    "train/test splits for tuning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'fraud-detection-data'\n",
    "DATA_PREFIX = 'processed'\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def load_parquet_from_s3(bucket: str, key: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a Parquet file from S3 into a pandas DataFrame.\"\"\"\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_parquet(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "\n",
    "train_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/train.parquet')\n",
    "test_df = load_parquet_from_s3(BUCKET_NAME, f'{DATA_PREFIX}/test.parquet')\n",
    "\n",
    "# Separate features and target\n",
    "TARGET = 'Class'\n",
    "FEATURES = [c for c in train_df.columns if c != TARGET]\n",
    "\n",
    "X_train = train_df[FEATURES]\n",
    "y_train = train_df[TARGET]\n",
    "X_test = test_df[FEATURES]\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "print(f'Training set:  {X_train.shape[0]:,} rows, {X_train.shape[1]} features')\n",
    "print(f'Test set:      {X_test.shape[0]:,} rows, {X_test.shape[1]} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "## 3. Initialize ExperimentTracker and HyperparameterTuner\n",
    "\n",
    "The `HyperparameterTuner` accepts an optional `ExperimentTracker` instance.\n",
    "When provided, every trial is automatically logged with its parameters and metrics\n",
    "(Requirement 3.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = ExperimentTracker(region_name='us-east-1')\n",
    "tuner = HyperparameterTuner(tracker=tracker)\n",
    "\n",
    "print('ExperimentTracker and HyperparameterTuner initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "## 4. Grid Search\n",
    "\n",
    "Grid search evaluates every combination in the parameter grid. This is thorough but\n",
    "can be expensive for large grids. Use it when the search space is small and you want\n",
    "full coverage.\n",
    "\n",
    "**Requirement 3.1**: Support grid search hyperparameter tuning with configurable parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'use_label_encoder': [False],\n",
    "    'eval_metric': ['logloss'],\n",
    "}\n",
    "\n",
    "total_combos = 1\n",
    "for v in param_grid.values():\n",
    "    total_combos *= len(v)\n",
    "print(f'Grid search will evaluate {total_combos} parameter combinations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = tuner.grid_search(\n",
    "    model_class=XGBClassifier,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    experiment_name='xgboost-grid-search',\n",
    "    scoring='accuracy',\n",
    ")\n",
    "\n",
    "print(f'Grid search complete — {len(grid_results[\"all_results\"])} trials evaluated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "### 4.1 Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a results DataFrame for easy inspection\n",
    "grid_rows = []\n",
    "for trial in grid_results['all_results']:\n",
    "    row = {**trial['params'], **trial['metrics'], 'score': trial['score']}\n",
    "    grid_rows.append(row)\n",
    "\n",
    "grid_df = pd.DataFrame(grid_rows).sort_values('score', ascending=False)\n",
    "grid_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Best Hyperparameters (Grid Search) ===')\n",
    "print(f'  Score (accuracy): {grid_results[\"best_score\"]:.4f}')\n",
    "for param, value in grid_results['best_params'].items():\n",
    "    print(f'  {param}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search: accuracy by max_depth and learning_rate\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, n_est in zip(axes, param_grid['n_estimators']):\n",
    "    subset = grid_df[grid_df['n_estimators'] == n_est]\n",
    "    pivot = subset.pivot_table(\n",
    "        index='max_depth', columns='learning_rate', values='accuracy'\n",
    "    )\n",
    "    sns.heatmap(pivot, annot=True, fmt='.4f', cmap='YlGnBu', ax=ax)\n",
    "    ax.set_title(f'n_estimators = {n_est}')\n",
    "\n",
    "plt.suptitle('Grid Search — Accuracy by max_depth and learning_rate', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "## 5. Random Search\n",
    "\n",
    "Random search samples from parameter distributions. It is more efficient than grid\n",
    "search for high-dimensional spaces because it explores a wider range of values.\n",
    "\n",
    "Distributions can be:\n",
    "- **Lists** — a random element is chosen uniformly\n",
    "- **Callables** (e.g. `lambda: random.uniform(0.01, 0.3)`) — called each iteration\n",
    "\n",
    "**Requirement 3.2**: Support random search hyperparameter tuning with configurable parameter distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': lambda: random.uniform(0.01, 0.3),\n",
    "    'n_estimators': lambda: random.randint(50, 200),\n",
    "    'subsample': lambda: random.uniform(0.5, 1.0),\n",
    "    'colsample_bytree': lambda: random.uniform(0.5, 1.0),\n",
    "    'use_label_encoder': [False],\n",
    "    'eval_metric': ['logloss'],\n",
    "}\n",
    "\n",
    "N_ITER = 15\n",
    "print(f'Random search will sample {N_ITER} parameter combinations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results = tuner.random_search(\n",
    "    model_class=XGBClassifier,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=N_ITER,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    experiment_name='xgboost-random-search',\n",
    "    scoring='accuracy',\n",
    ")\n",
    "\n",
    "print(f'Random search complete — {len(random_results[\"all_results\"])} trials evaluated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1",
   "metadata": {},
   "source": [
    "### 5.1 Random Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_rows = []\n",
    "for trial in random_results['all_results']:\n",
    "    row = {**trial['params'], **trial['metrics'], 'score': trial['score']}\n",
    "    random_rows.append(row)\n",
    "\n",
    "random_df = pd.DataFrame(random_rows).sort_values('score', ascending=False)\n",
    "random_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Best Hyperparameters (Random Search) ===')\n",
    "print(f'  Score (accuracy): {random_results[\"best_score\"]:.4f}')\n",
    "for param, value in random_results['best_params'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f'  {param}: {value:.4f}')\n",
    "    else:\n",
    "        print(f'  {param}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize random search: scatter of learning_rate vs accuracy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(random_df['learning_rate'], random_df['accuracy'], c='teal', alpha=0.7)\n",
    "axes[0].set_xlabel('learning_rate')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Random Search — Learning Rate vs Accuracy')\n",
    "\n",
    "axes[1].scatter(random_df['max_depth'], random_df['accuracy'], c='darkorange', alpha=0.7)\n",
    "axes[1].set_xlabel('max_depth')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Random Search — Max Depth vs Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3f4a5",
   "metadata": {},
   "source": [
    "## 6. SageMaker Automatic Model Tuning (Bayesian Optimization)\n",
    "\n",
    "For large-scale tuning, SageMaker Automatic Model Tuning uses Bayesian optimization\n",
    "to intelligently explore the hyperparameter space. It runs parallel training jobs on\n",
    "dedicated instances, making it faster for expensive models.\n",
    "\n",
    "**Requirement 3.3**: Support Bayesian optimization hyperparameter tuning using SageMaker Automatic Model Tuning\n",
    "\n",
    "> **Note**: This section requires a SageMaker execution role and S3 training data.\n",
    "> It will not run in a local-only environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner as SageMakerTuner,\n",
    ")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f'SageMaker session region: {region}')\n",
    "print(f'Execution role: {role}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the XGBoost estimator for SageMaker training\n",
    "xgb_image_uri = sagemaker.image_uris.retrieve('xgboost', region, version='1.5-1')\n",
    "\n",
    "xgb_estimator = Estimator(\n",
    "    image_uri=xgb_image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{BUCKET_NAME}/models/tuning-output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# Set static hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    ")\n",
    "\n",
    "print('XGBoost estimator configured.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter ranges for Bayesian optimization\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'colsample_bytree': ContinuousParameter(0.5, 1.0),\n",
    "    'num_round': IntegerParameter(50, 200),\n",
    "}\n",
    "\n",
    "print('Hyperparameter ranges:')\n",
    "for name, param_range in hyperparameter_ranges.items():\n",
    "    print(f'  {name}: {param_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Bayesian optimization via the project's HyperparameterTuner\n",
    "TRAIN_DATA_S3 = f's3://{BUCKET_NAME}/{DATA_PREFIX}/train.parquet'\n",
    "VALIDATION_DATA_S3 = f's3://{BUCKET_NAME}/{DATA_PREFIX}/validation.parquet'\n",
    "\n",
    "bayesian_results = tuner.bayesian_optimization(\n",
    "    estimator=xgb_estimator,\n",
    "    objective_metric_name='validation:auc',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=5,\n",
    "    train_data_s3=TRAIN_DATA_S3,\n",
    "    validation_data_s3=VALIDATION_DATA_S3,\n",
    ")\n",
    "\n",
    "print(f'Bayesian optimization complete.')\n",
    "print(f'  Best training job: {bayesian_results[\"best_training_job\"]}')\n",
    "print(f'  Tuning job name:   {bayesian_results[\"tuning_job_name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8e9f0",
   "metadata": {},
   "source": [
    "### 6.1 Retrieve Best Hyperparameters from SageMaker Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Best Hyperparameters (SageMaker Bayesian Optimization) ===')\n",
    "for param, value in bayesian_results['best_params'].items():\n",
    "    print(f'  {param}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0a1b2",
   "metadata": {},
   "source": [
    "## 7. Compare Tuning Methods\n",
    "\n",
    "Retrieve the best hyperparameters from each method side by side to decide which\n",
    "configuration to promote to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = {\n",
    "    'Grid Search': {\n",
    "        'best_score': grid_results['best_score'],\n",
    "        'best_params': grid_results['best_params'],\n",
    "        'trials': len(grid_results['all_results']),\n",
    "    },\n",
    "    'Random Search': {\n",
    "        'best_score': random_results['best_score'],\n",
    "        'best_params': random_results['best_params'],\n",
    "        'trials': len(random_results['all_results']),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f'{\"Method\":<20} {\"Best Score\":<15} {\"Trials\"}')\n",
    "print('-' * 50)\n",
    "for method, info in comparison.items():\n",
    "    print(f'{method:<20} {info[\"best_score\"]:<15.4f} {info[\"trials\"]}')\n",
    "\n",
    "# Determine overall winner between local methods\n",
    "winner = max(comparison, key=lambda m: comparison[m]['best_score'])\n",
    "print(f'\\nBest local method: {winner} (score: {comparison[winner][\"best_score\"]:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing best scores\n",
    "methods = list(comparison.keys())\n",
    "scores = [comparison[m]['best_score'] for m in methods]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(methods, scores, color=['steelblue', 'darkorange'])\n",
    "plt.ylabel('Best Accuracy')\n",
    "plt.title('Tuning Method Comparison — Best Accuracy')\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
    "             f'{score:.4f}', ha='center', va='bottom')\n",
    "plt.ylim(min(scores) - 0.01, max(scores) + 0.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e6",
   "metadata": {},
   "source": [
    "## 8. Query Experiment History\n",
    "\n",
    "Use the ExperimentTracker to retrieve logged experiments and inspect past tuning runs.\n",
    "\n",
    "**Requirement 3.4**: Log all parameter combinations and their performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all grid search experiments\n",
    "grid_experiments = tracker.query_experiments(\n",
    "    experiment_name='xgboost-grid-search'\n",
    ")\n",
    "\n",
    "print(f'Found {len(grid_experiments)} grid search experiments in tracker.')\n",
    "if grid_experiments:\n",
    "    print(f'Sample experiment keys: {list(grid_experiments[0].keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query random search experiments\n",
    "random_experiments = tracker.query_experiments(\n",
    "    experiment_name='xgboost-random-search'\n",
    ")\n",
    "\n",
    "print(f'Found {len(random_experiments)} random search experiments in tracker.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated three hyperparameter tuning strategies:\n",
    "\n",
    "1. **Grid Search** — exhaustive evaluation of all parameter combinations, best for small search spaces.\n",
    "2. **Random Search** — efficient sampling with callable distributions (lambdas), better for high-dimensional spaces.\n",
    "3. **SageMaker Automatic Model Tuning** — Bayesian optimization with parallel training jobs for large-scale tuning.\n",
    "\n",
    "All trials were logged to the ExperimentTracker for full reproducibility. The best\n",
    "hyperparameters from any method can be promoted to production using the\n",
    "`ProductionIntegrator` (see notebook 05).\n",
    "\n",
    "Next steps: compare algorithms (notebook 03) or promote the winning configuration to production (notebook 05)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
